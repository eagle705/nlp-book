# 언어모델의 다양한 아키텍쳐
## 인코더 모델
인코더 계열의 모델은 트랜스포머의 인코더 부분만을 활용합니다. 입력으로 들어오는 문장 내 모든 단어들을 고려할 수 있습니다. 이러한 모델들을 "양방향"(bi-directional) 어텐션을 갖는다고하며, *auto-encoding* 모델이라고도 표현합니다.

이러한 모델의 사전학습은 일반적으로 주어진 문장을 손상시키고 다시 복구하는 형태로 진행됩니다. 예컨데 문장내 임의의 단어를 마스킹하고 해당 단어를 예측하는 Masked Langauge Model이 이에 해당됩니다. 이러한 구조를 Denoising Autoencoder라고 말하기도 합니다.

인코더 모델의 경우 주로 문장분류, 개체명인식, 추출형 QA등의 태스크에 활용됩니다.
이러한 모델들은 다음과 같이 알려져있습니다.

- [ALBERT](https://huggingface.co/docs/transformers/model_doc/albert)
- [BERT](https://huggingface.co/docs/transformers/model_doc/bert)
- [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert)
- [ELECTRA](https://huggingface.co/docs/transformers/model_doc/electra)
- [RoBERTa](https://huggingface.co/docs/transformers/model_doc/roberta)

## 디코더 모델
디코더 모델들은 트랜스포머의 디코더 부분만을 활용합니다. 입력으로 들어오는 문장 내에서 주어진 단어 이전에 위치한 단어들까지만 고려할 수 있습니다. 이러한 모델들을 *auto-regressive* 모델이라고도 표현합니다.

이러한 모델의 사전학습은 일반적으로 문장내에서 다음 단어를 예측하는 형태로 진행됩니다.
디코더 모델은 주로 텍스트생성 태스크에 활용됩니다.
이러한 모델들은 다음과 같이 알려져있습니다.

- [CTRL](https://huggingface.co/docs/transformers/model_doc/ctrl)
- [GPT](https://huggingface.co/transformers/model_doc/gpt.html)
- [GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2)
- [TransformerXL](https://huggingface.co/docs/transformers/model_doc/transformerxl)

## 인코더-디코더 모델
인코더-디코더 모델, 다른 말로 sequence-to-sequence (seq2seq) 모델은 transformer 아키텍쳐와 동일하게 인코더 디코더를 모두 사용합니다. 위에서 설명한것과 마찬가지로 인코더에서는 입력의 모든 단어를 고려하고, 디코더에서는 주어진 단어 앞까지의 단어를 고려할 수 있습니다.

이러한 모델의 사전학습은 인코더 디코더의 목적함수를 사용해서 진행할 수 있지만, 모델 구조에 따라 좀 더 다양한 방법으로 수행됩니다. 예를 들어 T5 모델은 인코더에서 임의의 텍스트 범위를 단일 마스크 토큰으로 대체하고, 디코더에서는 이 마스크 단어를 대체할 텍스트를 예측합니다.

seq2seq 모델은 요약, 번역, generative QA와 같이 주어진 입력에 따라 새로운 문장을 생성하는 태스크에 활용됩니다.
이러한 모델은 다음과 같이 알려져있습니다.

- [BART](https://huggingface.co/docs/transformers/model_doc/bart)
- [T5](https://huggingface.co/docs/transformers/model_doc/t5)