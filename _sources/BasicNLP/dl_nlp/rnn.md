# RNN
RNN은 Recurrent Neural Network의 약자로, 자연어 처리(NLP) 분야에서 널리 사용되는 신경망 모델입니다. RNN은 순차적으로 들어오는 입력 데이터를 처리하면서, 이전 시간(time step)에서의 정보를 기억하고 이를 현재 상태에 반영하여 출력을 만들어냅니다.

RNN은 일반적으로 시퀀스 데이터를 처리하기 위해 사용됩니다. 예를 들어, 문장, 문서, 음성 신호, 동영상 등이 시퀀스 데이터의 예입니다. 이러한 시퀀스 데이터는 각각의 요소(단어, 프레임, 음성 신호 샘플 등)가 이전 요소와 상호작용하면서 의미를 갖는 특성이 있습니다.

RNN은 이전 시간의 출력값이 현재 시간의 입력값으로 사용되는 피드백 루프(Feedback Loop)를 가지고 있습니다. 이를 통해, RNN은 이전 상태의 정보를 현재 상태에서 고려할 수 있게 됩니다. 이전 시간의 출력값은 현재 시간의 입력과 함께 새로운 출력값을 생성하고, 이 과정이 반복되면서 시퀀스 데이터를 처리합니다.

RNN의 가장 일반적인 구조는 LSTM(Long Short-Term Memory)입니다. LSTM은 RNN의 단점인 장기 의존성 문제를 해결하기 위해 고안된 모델로, 중요한 정보를 오랫동안 기억할 수 있도록 설계되었습니다. LSTM은 입력 게이트, 삭제 게이트, 출력 게이트 등의 메커니즘을 통해 입력 데이터의 흐름을 조절하고, 필요한 정보를 저장하거나 삭제합니다.

LSTM과 함께 유명한 네트워크는 GRU입니다. GRU는 Gated Recurrent Unit의 약자로, LSTM과 마찬가지로 RNN의 한 종류입니다. LSTM과 비슷한 역할을 하지만, LSTM보다 간단한 구조를 가지고 있습니다.

GRU는 LSTM과 마찬가지로 장기 의존성 문제를 해결하기 위해 고안되었습니다. GRU는 LSTM보다 더 간단한 구조를 가지고 있어서, 학습이 더 빠르고 메모리 사용량이 적습니다.

GRU는 LSTM과 같이 입력 게이트와 출력 게이트를 가지고 있습니다. 그러나 LSTM과 달리, GRU는 두 개의 게이트가 아니라 업데이트 게이트와 리셋 게이트 두 개의 게이트를 가지고 있습니다.

업데이트 게이트는 이전 시간의 상태를 얼마나 유지할지를 결정하는 역할을 하며, 리셋 게이트는 현재 입력을 얼마나 무시할지를 결정합니다. 이러한 게이트들은 현재 입력과 이전 상태의 조합으로 계산됩니다.

GRU는 LSTM보다 조금 더 간단한 구조를 가지고 있지만, LSTM과 유사한 성능을 발휘합니다. GRU는 LSTM보다 더 빠르게 학습할 수 있으며, 메모리 사용량도 적기 때문에, 대규모 데이터셋을 다룰 때 유용합니다.

GRU도 LSTM과 마찬가지로 자연어 처리(NLP) 분야에서 다양한 응용에 사용됩니다. 예를 들어, 언어 모델링, 기계 번역, 문장 생성, 감성 분석, 자동 요약 등의 작업에 적용됩니다.
