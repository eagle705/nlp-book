# Probability Theory

Probability theory(확률 이론)은 언어 모델을 이해하는 데 중요한 수학적 기초 지식입니다. 확률 이론은 어떤 사건이 일어날 가능성을 수학적으로 계산하는 이론입니다.

예를 들어, 동전 던지기는 간단한 확률 문제입니다. 동전을 던져서 앞면이 나올 확률과 뒷면이 나올 확률은 모두 1/2입니다. 이를 확률 변수(random variable)로 나타낼 수 있습니다. 동전의 앞면을 1, 뒷면을 0으로 정의하면, 이 확률 변수는 Bernoulli distribution(베르누이 분포)를 따릅니다. Bernoulli distribution은 0 또는 1의 두 가지 값 중 하나를 가질 확률 변수를 모델링하는 데 사용됩니다.

또한, 여러 개의 동전을 던지는 경우에는 Binomial distribution(이항 분포)를 사용할 수 있습니다. 예를 들어, 10개의 동전을 던져서 앞면이 6번 나올 확률은 이항 분포를 사용하여 계산할 수 있습니다.

또 다른 예시로, 특정 단어가 나타날 확률을 계산하는 경우가 있습니다. 이를 위해서는 Maximum Likelihood Estimation(최대 우도 추정)을 사용합니다. 예를 들어, 어떤 문서에서 'apple'이라는 단어가 나타날 확률을 계산하려면, 해당 문서에서 'apple'이 나타난 횟수를 셈으로써 확률을 추정할 수 있습니다. 이를 Maximum Likelihood Estimation을 통해 계산할 수 있습니다.

이러한 확률 이론의 개념들은 언어 모델링에 적용됩니다. 언어 모델은 이전 단어들이 주어졌을 때 다음 단어가 나타날 확률을 예측하는 모델입니다. 이를 위해서는 여러 가지 확률 분포를 사용할 수 있으며, 이를 확률 모델(probabilistic model)이라고 합니다. 예를 들어, n-gram 기반 언어 모델은 다음 단어의 확률을 이전 n-1개의 단어로부터 예측합니다. 이를 위해 n-gram 기반 언어 모델에서는 각 단어가 나타날 확률을 추정하기 위해 Maximum Likelihood Estimation을 사용합니다.

Maximum Likelihood Estimation(MLE)은 확률분포를 추정하기 위한 방법 중 하나로, 어떤 확률분포에서 관측된 데이터를 보고 그 확률분포의 모수(parameter) 값을 추정하는 방법입니다.



---- 아래 내용 검토 필요 ----


예를 들어, 동전 던지기에서 동전을 10번 던져서 7번 앞면이 나왔다면, 이 데이터를 이용해 동전의 앞면이 나올 확률을 추정할 수 있습니다. 이 경우, 앞면이 나올 확률을 나타내는 모수를 $\theta$라 하면, MLE는 이 데이터에서 가장 가능성이 높은 $\theta$값을 추정하는 것입니다.

확률(probability)과 우도(likelihood)는 비슷한 개념이지만, 조금씩 다릅니다. 확률은 이미 알고 있는 모수값($\theta$)을 가지고 어떤 사건이 발생할 확률을 계산하는 것이며, 우도는 이미 발생한 사건을 보고 모수값이 얼마나 그 사건을 잘 설명하는지를 나타내는 값입니다.

예를 들어, 동전 던지기 문제에서, $\theta=0.7$일 때, 앞면이 나올 확률은 $P(X=1|\theta=0.7)=0.7$입니다. 이때, 10번 던져서 7번 앞면이 나왔다면, 이 데이터의 우도는 다음과 같이 계산됩니다.

$$
\begin{aligned}
L(\theta \mid X) & =P(X \mid \theta) \\
& =P\left(X_1=1, X_2=1, \ldots, X_{10}=0 \mid \theta\right) \\
& =\prod_{i=1}^{10} P\left(X_i \mid \theta\right) \\
& =\theta^7(1-\theta)^3
\end{aligned}
$$
 
이 경우, $\theta=0.7$일 때, $L(\theta=0.7|X)$가 가장 큰 값을 가지므로, MLE는 $\theta=0.7$이라고 추정합니다. 이것이 동전의 앞면이 나올 확률을 추정하는 가장 간단한 MLE의 예시입니다.

우도는 다음과 같이 계산됩니다.

$$\mathcal{L}(p|x) = P(X=x|p) = p^x(1-p)^{n-x}$$

여기서 $p$는 앞면이 나올 확률, $n$은 시행 횟수, $x$는 앞면이 나온 횟수를 나타냅니다.


참고로, 확률의 모수값과 확률의 사전확률(prior)은 다른 개념입니다.

확률의 모수값은 분포의 특성을 결정하는 값으로, 예를 들어 베르누이 분포에서의 모수값은 앞면이 나올 확률, 가우시안 분포에서의 모수값은 평균과 표준편차입니다.

반면에 확률의 사전확률(prior)은 확률 분포를 결정하는 모수값이 어떤 값일 가능성이 높은지를 나타내는 분포입니다. 사전확률은 모수값이 가질 수 있는 가능성을 나타내는 것으로, 모수값에 대한 불확실성을 표현합니다.

예를 들어, 베이지안 추정에서는 사전확률을 이용하여 데이터를 바탕으로 모수값을 추정하는데, 이때 모수값이 가질 수 있는 가능성이 높은 영역을 사전확률이 나타냅니다. 따라서 모수값과 사전확률은 서로 다른 개념입니다.


----

동전 던지기 문제에서 앞면이 나올 확률 $\theta$를 MLE를 사용하여 추정하는 과정을 예시로 설명해보겠습니다.

예를 들어, 동전을 100번 던져서 60번이 앞면이 나왔다고 가정해봅시다. 이때, MLE를 사용하여 $\theta$를 추정하는 과정은 다음과 같습니다.

MLE를 사용하여 추정하려는 것은 앞면이 나올 확률 $\theta$입니다. 이때, 앞면이 나오는 경우를 1, 뒷면이 나오는 경우를 0으로 표시하면 각각의 결과는 베르누이 분포를 따르게 됩니다. 이때, 동전을 100번 던졌으므로 이것은 독립적인 베르누이 시행 100번을 수행한 것과 같습니다.

따라서, 우리가 관측한 데이터 $D$는 다음과 같은 확률 분포를 따릅니다.

$$P(D|\theta) = \theta^{60}(1-\theta)^{40}$$

이때, MLE를 사용하여 $\theta$를 추정하기 위해서는 이 확률 분포를 최대화하는 $\theta$를 찾으면 됩니다. 확률 분포를 최대화하는 $\theta$를 찾기 위해서는 미분을 이용한 최적화 방법 등을 사용할 수 있습니다.

위의 확률 분포를 로그 변환하여 최대화하는 것이 일반적입니다. 로그 변환을 하면 다음과 같은 식이 됩니다.

$$\log P(D|\theta) = 60\log\theta + 40\log(1-\theta)$$

이 식을 $\theta$로 미분하고, 그 값이 0이 되는 $\theta$를 찾으면 됩니다. 미분한 결과는 다음과 같습니다.

$$\frac{d}{d\theta} \log P(D|\theta) = \frac{60}{\theta} - \frac{40}{1-\theta}$$

이 값을 0으로 놓고, $\theta$에 대해 정리하면 다음과 같습니다.

$$\theta = \frac{60}{100} = 0.6$$

따라서, 이 동전을 던졌을 때 앞면이 나올 확률은 0.6이라고 추정할 수 있습니다.

----
언어모델 관점에서 MLE 예시를 들어보겠습니다. 예를 들어, 문장 "The cat is on the mat"이 있을 때, 이 문장이 언어모델 $P$에서 나올 확률 $P(\text{"The cat is on the mat"})$을 MLE를 이용하여 추정해보겠습니다.

우선, $P(\text{"The cat is on the mat"})$는 다음과 같이 표현할 수 있습니다.

$$P(\text{"The cat is on the mat"}) = P(\text{"The"}) \times P(\text{"cat"}|\text{"The"}) \times P(\text{"is"}|\text{"The cat"}) \times P(\text{"on"}|\text{"The cat is"}) \times P(\text{"the"}|\text{"The cat is on"}) \times P(\text{"mat"}|\text{"The cat is on the"})$$

MLE를 이용하여 각 확률 값을 추정하기 위해서는, 주어진 데이터셋에서 각각의 단어들이 몇 번 등장하는지를 셀 수 있어야 합니다. 이를 위해, 예를 들어 대규모의 코퍼스에서 모든 단어의 등장 횟수를 세어서 이를 바탕으로 각각의 확률 값을 추정할 수 있습니다.

예를 들어, $P(\text{"The"})$는 "The"가 전체 문장에서 몇 번 등장하는지를 셈으로써 추정할 수 있습니다. 비슷하게, $P(\text{"cat"}|\text{"The"})$는 "The" 다음에 "cat"이 등장하는 빈도수를 "The"의 빈도수로 나누어서 추정할 수 있습니다. 이러한 방식으로, 각각의 확률 값을 추정할 수 있습니다.

하지만, 이 방법은 희소성(sparsity) 문제와 같은 다양한 문제점을 가지고 있습니다. 희소성 문제란, 예를 들어 특정한 단어나 구절이 코퍼스에서 드물게 등장하는 경우, 해당 단어나 구절의 확률 값을 정확하게 추정하기 어렵다는 문제를 의미합니다. 이러한 문제를 해결하기 위해서는 smoothing과 같은 방법을 사용해야 합니다.
