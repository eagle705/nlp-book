# Language Model
언어 모델(Language Model)은 기계 학습 분야에서 자연어 처리(Natural Language Processing) 작업에 사용되는 알고리즘입니다. 언어 모델은 주어진 단어 시퀀스의 확률을 예측하도록 학습되며, 다음 단어 예측, 기계 번역, 음성 인식 등과 같은 여러 자연어 처리 작업에서 중요한 역할을 합니다.

언어 모델은 일반적으로 통계 모델링 방법을 사용하여 작동합니다. 가장 간단한 언어 모델은 n-gram 모델로, 이전 n-1개의 단어를 보고 다음 단어를 예측하는데 사용됩니다. 예를 들어, 3-gram 언어 모델은 이전 두 단어를 보고 다음 단어를 예측하는데 사용됩니다.

언어 모델의 학습은 주어진 텍스트 데이터에서 확률 분포를 추정하는 과정입니다. 확률 분포는 각 단어 시퀀스에 대한 확률 값을 할당하며, 모델은 주어진 텍스트 데이터에서 학습됩니다. 일반적으로 최대 우도 추정(Maximum Likelihood Estimation) 방법을 사용하여 모델의 매개 변수를 학습합니다.

수식으로 나타내면, 언어 모델은 다음과 같이 표현될 수 있습니다.

$$ P(w_1, w_2, \dots, w_n) = \prod_{i=1}^{n} P(w_i | w_{i-1}, \dots, w_1) $$

여기서 $w_1, w_2, \dots, w_n$은 단어 시퀀스이고, $P(w_i | w_{i-1}, \dots, w_1)$은 이전 $i-1$개의 단어를 고려하여 $w_i$의 확률을 나타냅니다. 

n-gram 모델에서는 Markov 가정을 사용하여, 이전 n-1개의 단어가 주어졌을 때 현재 단어와 이전 n-1개의 단어들 사이에 조건부 독립성을 가정합니다. 이러한 가정으로 인해 조건부 확률 $P(w_i|w_{i-1},\dots,w_1)$을 다음과 같이 간단하게 계산할 수 있습니다.

$$ P(w_i|w_{i-1},\dots,w_1) \approx P(w_i|w_{i-1},\dots,w_{i-n+1}) $$

즉, 현재 단어 $w_i$의 확률은 이전 n-1개의 단어 $w_{i-1},\dots,w_{i-n+1}$에만 의존하게 됩니다. 이러한 가정을 통해 계산량을 줄이고 모델의 복잡도를 낮추면서도, 어느 정도의 성능을 보장할 수 있습니다.

 Markov 가정은 실제 언어의 특성과는 부분적으로 일치하지만, 일반적으로 자연어 처리에서 사용되는 다양한 태스크에서 좋은 성능을 보이는 것으로 알려져 있습니다.

하지만, Markov 가정이 완벽하게 맞지 않는 경우도 있습니다. 예를 들어, 문맥이 길어질수록 조건부 독립성 가정이 더 이상 성립하지 않을 수 있습니다. 이러한 경우에는 더 긴 문맥을 고려하는 다른 모델링 기법이 더 나은 성능을 보일 수 있습니다.

따라서, n-gram 기반 언어 모델은 단순하면서도 효과적인 모델링 방법이지만, 언제나 최상의 모델링 방법이 아니며, 태스크와 데이터에 따라 다른 모델링 방법이 더 나은 성능을 보일 수 있습니다.

언어모델의 확률은 일반적으로 신경망 모델 등을 사용하여 추정할 수도 있습니다.

신경망 모델은 언어 모델링을 위해 다양한 구조를 사용할 수 있습니다. 대표적인 구조로는 순환 신경망(RNN, Recurrent Neural Network), 장단기 기억 셀(LSTM, Long Short-Term Memory), 게이트 순환 유닛(GRU, Gated Recurrent Unit) 등이 있습니다.

언어 모델링을 위해 이러한 신경망 모델은 입력 시퀀스와 이전 상태를 입력으로 받아, 다음 단어를 예측하는 출력 값을 생성합니다. 이 출력 값은 다시 다음 시점에 입력으로 사용됩니다. 이러한 과정을 반복하여 시퀀스의 각 단어를 예측합니다.

예를 들어, 다음과 같은 문장이 있다고 가정해보겠습니다.

"I have a cat."

여기서 RNN 기반 언어 모델은 이전 시점에 입력된 단어들을 고려하여 다음 단어 "cat"을 예측하는 확률 분포를 출력합니다.

$$ P(\text{"cat"} | \text{"I have a"}) $$

이 확률 분포는 출력층의 뉴런 수와 각 단어의 임베딩 벡터 차원 수와 같은 하이퍼파라미터에 의해 결정됩니다. 출력층의 뉴런 수가 단어 집합(vocabulary)의 크기와 같다면, 출력층의 각 뉴런은 각 단어에 대한 확률을 출력하게 됩니다. 즉, 다음과 같이 표현할 수 있습니다.

$$ P(\text{word} | \text{context}) = \frac{\text{exp}(\text{score}(\text{context}, \text{word}))}{\sum_{w \in \text{vocabulary}} \text{exp}(\text{score}(\text{context}, w))} $$

여기서 $score(context, word)$는 주어진 입력(context)과 단어(word)의 점수(score)를 나타냅니다. 이 점수는 주어진 입력과 단어에 대한 조건부 확률 값을 계산하는 데 사용됩니다.

이렇게 계산된 확률 분포를 기반으로 모델은 다음 단어를 예측합니다. 예를 들어, 위의 확률 분포에서 "cat"의 확률이 가장 높다면, 모델은 다음 단어로 "cat"을 예측합니다.

n-gram 모델에서 사용하는 수식과 신경망에서 사용하는 수식의 차이는 다음과 같습니다.

n-gram 모델의 수식은 조건부 확률의 곱으로서, 각 단어의 확률은 이전 n-1개의 단어에만 의존하고 그 이상의 단어는 고려하지 않습니다. 이 때문에 이 모델은 long-term dependency를 고려하지 않아서 문장 전체의 확률을 정확하게 모델링하기 어렵습니다.

반면에, 신경망 기반 언어 모델은 입력으로 주어진 모든 단어를 이용해 다음 단어의 확률 분포를 계산합니다. 따라서, 이 모델은 long-term dependency를 고려할 수 있으며, 보다 정확한 문장 전체의 확률을 계산할 수 있습니다.

신경망 모델에서는 다음 단어의 확률을 조건부 확률 $P(w_i | w_{i-1}, \dots, w_1)$으로 표현합니다. 이 확률은 입력으로 주어진 모든 단어들 $w_1, w_2, \dots, w_{i-1}$의 임베딩 벡터와 이전 시점의 hidden state를 입력으로 받아 계산됩니다. 이때 hidden state는 모델이 입력을 처리하면서 내부적으로 유지되는 상태 벡터로서, 모델이 이전에 처리한 입력에 대한 정보를 포함하고 있습니다. 따라서, 이전 시점의 hidden state를 입력으로 받아 계산되는 $P(w_i | w_{i-1}, \dots, w_1)$은 모델이 이전 단어들의 정보를 활용해 다음 단어의 확률을 예측하게 됩니다. 이때, 확률 분포의 출력층은 보통 softmax 함수를 사용하여 각 단어에 대한 확률을 계산하게 됩니다.

언어모델의 성능은 Perplexity(혼란도)로 측정할 수 있습니다.
Perplexity(ppl)는 언어 모델의 성능을 평가하는 지표 중 하나입니다. Perplexity는 모델이 주어진 문장을 얼마나 잘 예측하는지를 나타내며, 작을수록 모델의 성능이 더 좋습니다.

수식으로는 다음과 같이 정의됩니다.

$$ PPL(w_1, w_2, \dots, w_N) = \sqrt[N]{\frac{1}{P(w_1, w_2, \dots, w_N)}} $$

여기서 $P(w_1, w_2, \dots, w_N)$는 모델이 예측한 문장의 확률이고, $N$은 문장의 길이입니다. 즉, Perplexity는 주어진 문장의 확률의 역수의 geometric mean으로 정의됩니다. 따라서 Perplexity는 일종의 평균 분기 수(break-even point)로 이해할 수 있습니다. 예를 들어, Perplexity가 2인 모델은 각 단어마다 2개의 후보 단어가 있을 때, 모델이 선택한 단어가 정답일 확률이 50%인 모델입니다.

Perplexity는 모델의 예측 정확도를 나타내는 지표이므로, 작을수록 더 좋은 모델입니다. Perplexity가 1에 가까울수록 모델은 입력된 문장의 확률을 거의 100%로 예측할 수 있는 뛰어난 성능을 가진 모델입니다. 반면에 Perplexity가 높을수록 모델의 예측이 부정확하며, 높은 Perplexity는 모델의 예측이 불확실한 정도가 크다는 것을 나타냅니다.
