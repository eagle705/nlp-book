# PEFT(Parameter Efficient Fine-Tuning)

ì•ˆë…•í•˜ì„¸ìš”, ì˜¤ëŠ˜ì€ PEFT(Parameter Efficient Fine-Tuning)ì´ë¼ê³  ë¶ˆë¦¬ìš°ëŠ” ê¸°ë²•ì— ëŒ€í•´ì„œ ì•Œì•„ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. PEFTëŠ” ì§§ê²Œ ì„¤ëª…í•˜ë©´ ëª¨ë¸ì˜ ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ íŠœë‹í•˜ëŠ” ê²ƒì´ ì•„ë‹Œ ì¼ë¶€ íŒŒë¼ë¯¸í„°ë§Œì„ íŠœë‹í•¨ìœ¼ë¡œì¨ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì ì€ ìžì›ìœ¼ë¡œë„ ë†’ê²Œ ìœ ì§€í•˜ëŠ” ë°©ë²•ë¡ ìž…ë‹ˆë‹¤. PEFT ê¸°ë²•ì¤‘ì—ì„œ ê°€ìž¥ ë§Žì´ ì•Œë ¤ì§„ ê²ƒì€ LoRAë¼ëŠ” ê¸°ë²•ì¸ë°ìš”. ì˜¤ëŠ˜ì€ LoRAì— ëŒ€í•´ì„œ ê°„ë‹¨í•˜ê²Œ ë°°ìš°ê³  ë” ë‚˜ì•„ê°€ IA3ë¼ëŠ” ë°©ë²•ë¡ ì— ëŒ€í•´ì„œë„ ì§§ê²Œ ë¦¬ë·°í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

## ë¼ì§€ ì–¸ì–´ ëª¨ë¸ì˜ ë°œì „ê³¼ PEFTì˜ í•„ìš”ì„±
ìµœê·¼ GPT-3(175B)ì™€ ê°™ì€ ë§¤ìš° í° ì–¸ì–´ëª¨ë¸ì´ ë“±ìž¥í•¨ì— ë”°ë¼ ì–¸ì–´ëª¨ë¸ì— í•´ê²°í•˜ê³ ìž í•˜ëŠ” ë¬¸ì œì— ëŒ€í•œ ëª‡ê°€ì§€ ì˜ˆì‹œë§Œ few-shotìœ¼ë¡œ ë¯¸ë¦¬ ìž…ë ¥ì„ í•´ì£¼ë©´ in-context learning(ICL)ì— ì˜í•´ ëª¨ë¸ì„ ë”°ë¡œ íŠœë‹í•  í•„ìš” ì—†ì´ ë¬¸ì œë¥¼ ì‰½ê²Œ í•´ê²°í•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•´ì¡ŒìŠµë‹ˆë‹¤. 

![image](https://user-images.githubusercontent.com/7252598/232743915-d6b309c5-6029-4a30-b9f2-3062206cac44.png)

ê·¸ëŸ¬ë‚˜ ì´ëŸ¬í•œ ICLì€ ë§¤ë²ˆ ë¯¸ë¦¬ ì˜ˆì œë¥¼ ìž…ë ¥í•´ì£¼ì–´ì•¼í•˜ê¸° ë•Œë¬¸ì— ê³„ì‚°ì ì¸ ë¶€ë¶„ì´ë‚˜ ë©”ëª¨ë¦¬, ì €ìž¥ë¹„ìš©ë“¤ì´ ë°œìƒí•˜ê²Œ ëœë‹¤ëŠ” ë‹¨ì ì´ ìžˆìŠµë‹ˆë‹¤. ë˜í•œ ì–´ë–¤ ì—°êµ¬ì—ì„œëŠ” incorrect labelsì„ ì˜ˆì œë¡œ ë„£ì–´ì£¼ë”ë¼ë„ ë¬¸ì œë¥¼ ìž˜ í•´ê²°í•˜ê¸°ë„ í•˜ë”ë¼ ë¼ëŠ” ë‚´ìš©ë„ ìžˆì–´ì„œ ICLì˜ ê²°ê³¼ë¥¼ ì˜¨ì „ížˆ ì‹ ë¢°í•˜ê¸° ì–´ë µë‹¤ ë¼ëŠ” ì˜ê²¬ë„ ìžˆìŠµë‹ˆë‹¤.   

ì˜¤ëŠ˜ ë‹¤ë£¨ê²Œ ë˜ëŠ” `PEFT(Parameter Efficient Fine-Tuning)`ì€ ì´ëŸ¬í•œ ë‹¨ì ì„ ë³´ì™„í•˜ëŠ” ëŒ€ì•ˆì ì¸ íŒ¨ëŸ¬ë‹¤ìž„ì¤‘ í•˜ë‚˜ì´ê³  ì ì€ì–‘ íŒŒë¼ë¯¸í„°(ì˜ˆë¥¼ ë“¤ë©´ 0.01%)ë¥¼ í•™ìŠµí•¨ìœ¼ë¡œì¨ ë¹ ë¥¸ ì‹œê°„ ë‚´ì— ìƒˆë¡œìš´ ë¬¸ì œë¥¼ ê±°ì˜ ë¹„ìŠ·í•œ ì„±ëŠ¥ìœ¼ë¡œ í’€ ìˆ˜ ìžˆê²Œ í•˜ìžëŠ”ê²Œ ì£¼ëœ ëª©í‘œìž…ë‹ˆë‹¤. ì–¸ì–´ëª¨ë¸ì²˜ëŸ¼ ë§¤ìš° ë§Žì€ ìˆ˜ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì“°ëŠ” ëª¨ë¸ì´ ì‚¬ì‹¤ì€ ì ì€ ìˆ˜ì˜ íŒŒë¼ë¯¸í„°ë¥¼ íŠœë‹í•´ë„ ë¹„ìŠ·í•œ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìžˆë‹¤ëŠ” ì„ í–‰ì—°êµ¬ë“¤ì´ ìžˆì—ˆê³  ì´ëŸ¬í•œ ì—°êµ¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í˜„ìž¬ PEFTë¥¼ ìœ„í•œ ë‹¤ì–‘í•œ ë°©ë²•ë¡ ë“¤ì´ ì—°êµ¬ë˜ê³  ìžˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬í•œ ë°©ë²•ë¡ ì¤‘ì—ì„œ ê°€ìž¥ ìœ ëª…í•œ ê²ƒì¤‘ í•˜ë‚˜ê°€ LoRAì´ê³ , ìµœê·¼ì—ëŠ” LoRAë¥¼ ê°œì„ í•œ ë°©ë²•ë¡ ë“¤ë„ ë§Žì´ ë‚˜ì˜¤ê³  ìžˆëŠ” ìƒí™©ìž…ë‹ˆë‹¤. ì´ë²ˆì—ëŠ” LoRAì— ëŒ€í•´ì„œ ê°„ë‹¨í•˜ê²Œ ë¦¬ë·°í•˜ê³  IA3ë¼ëŠ” ë” ê°œì„ ëœ ë°©ë²•ì— ëŒ€í•´ì„œë„ ì‚´íŽ´ë³´ë ¤í•©ë‹ˆë‹¤. í˜„ì‹œì  ê¸°ì¤€ìœ¼ë¡œ LoRAëŠ” [Huggingfaceì—ì„œ ê³µê°œí•œ PEFT ë¼ì´ë¸ŒëŸ¬ë¦¬](https://github.com/huggingface/peft)ì—ì„œ êµ¬í˜„ì´ ë˜ì–´ìžˆê³  IA3ì˜ ê²½ìš° [NVIDIA NeMo](https://github.com/NVIDIA/NeMo/commit/cb2793c0c7bb352e1dfd8c349a96efc1dd260179)ì— êµ¬í˜„ì´ ë˜ì–´ìžˆìŠµë‹ˆë‹¤.

## PEFT ê¸°ë²•ë“¤
ì´ˆê¸°ì— PEFTì„ ìœ„í•´ ì œì•ˆë˜ì—ˆë˜ ë°©ë²•ì€ ì–´ëŒ‘í„°(`adapters`)ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒìž…ë‹ˆë‹¤. ì—¬ê¸°ì„œ ë§í•˜ëŠ” `adapater`ëž€ ê¸°ì¡´ì— ì´ë¯¸ í•™ìŠµì´ ì™„ë£Œëœ ëª¨ë¸(pre-trained model)ì˜ ì‚¬ì´ì‚¬ì´ì— í•™ìŠµ ê°€ëŠ¥í•œ ìž‘ì€ feed-forward networksë¥¼ ì‚½ìž…í•˜ëŠ” êµ¬ì¡°ë¥¼ ë§í•©ë‹ˆë‹¤. ì´ë•Œ pre-trained modelì˜ weightsëŠ” ê³ ì •í•´ë†“ê³  í•™ìŠµ ê°€ëŠ¥í•œ ìž‘ì€ feed-forward networksë§Œ ì•„í‚¤í…ì³ ì¤‘ê°„ì¤‘ê°„ë§ˆë‹¤ ì¶”ê°€í•¨ìœ¼ë¡œì¨ ì ì€ ìˆ˜ì˜ íŒŒë¼ë¯¸í„°ë¡œ ëª¨ë¸ì„ íŠœë‹í•˜ëŠ” ê¸°ë²•ìž…ë‹ˆë‹¤.   
ì´ëŸ¬í•œ ì–´ëŒ‘í„°ê¸°ë°˜ì˜ ë°©ë²•ë¡  ì™¸ì—ë„ `LoRA`, `prompt tuning`, `prefix tuning`ë“± ë‹¤ì–‘í•œ ë°©ë²•ë¡ ì´ ì œì•ˆë˜ì—ˆìŠµë‹ˆë‹¤.   
ì—¬ëŸ¬ ë°©ë²•ë¡ ì´ ìžˆì§€ë§Œ Stable diffusionì´ë‚˜ LLaMA, [Alpaca](https://devocean.sk.com/search/techBoardDetail.do?ID=164659)ì—ì„œë„ ë§Žì´ ì ìš©ë˜ëŠ” Microsoftì—ì„œ ê³µê°œí•œ `LoRA`ë¼ëŠ” ë°©ë²•ë¡ ì´ í˜„ìž¬ë¡œì„œëŠ” ì œì¼ ìœ ëª…í•œ ê²ƒ ê°™ìŠµë‹ˆë‹¤.


## LoRA
LoRA(**Lo**w-**R**ank **A**daptation)ì˜ ê°œë…ì„ ê°„ë‹¨í•˜ê²Œ ì„¤ëª…í•˜ìžë©´, ê³ ì •ëœ weightsë¥¼ ê°–ëŠ” pretrained modelì— í•™ìŠµì´ ê°€ëŠ¥í•œ rank decomposition í–‰ë ¬ì„ ì‚½ìž…í•œê²ƒìœ¼ë¡œ ì¤‘ê°„ì¤‘ê°„ í•™ìŠµì´ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ë¥¼ ì‚½ìž…í–ˆë‹¤ëŠ” ì ì—ì„œëŠ” ì–´ëŒ‘í„°ì™€ ë¹„ìŠ·í•˜ì§€ë§Œ êµ¬ì¡°ì ìœ¼ë¡œ ì¡°ê¸ˆ ë‹¤ë¥´ë‹¤ê³  í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
ì ì€ ì–‘ì˜ íŒŒë¼ë¯¸í„°ë¡œ ëª¨ë¸ì„ íŠœë‹í•˜ëŠ” ë°©ë²•ë¡ ì´ê¸° ë•Œë¬¸ì— ì ì€ìˆ˜ì˜ GPUë¡œ ë¹ ë¥´ê²Œ íŠœë‹í•  ìˆ˜ ìžˆë‹¤ëŠ” ìž¥ì ì´ ìžˆìŠµë‹ˆë‹¤. LoRAì—ì„œ ë‚˜ì˜¨ rank decompositionì´ë¼ëŠ” ë§ì´ ì²˜ìŒì—ëŠ” ì–´ë µê²Œ ëŠê»´ì¡Œì—ˆëŠ”ë°ìš”. ì•„ëž˜ ë³´ì´ëŠ” ê·¸ë¦¼ì—ì„œ ì²˜ëŸ¼ í–‰ë ¬ì˜ ì°¨ì›ì„  `r` ë§Œí¼ ì¤„ì´ëŠ” í–‰ë ¬ê³¼ ë‹¤ì‹œ ì›ëž˜ í¬ê¸°ë¡œ í‚¤ì›Œì£¼ëŠ” í–‰ë ¬ì˜ ê³±ìœ¼ë¡œ ë‚˜íƒ€ë‚´ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

![LoRA](https://user-images.githubusercontent.com/7252598/230259439-fe58295d-9879-41c8-9454-0ecbe27cacde.png)

ìœ„ ê·¸ë¦¼ì²˜ëŸ¼ ë ˆì´ì–´ ì¤‘ê°„ì¤‘ê°„ë§ˆë‹¤ ì¡´ìž¬í•˜ëŠ” hidden states `h`ì— ê°’ì„ ë”í•´ì¤„ìˆ˜ ìžˆëŠ” íŒŒë¼ë¯¸í„°ë¥¼ ì¶”ê°€í•´ì¤˜ì„œ ëª¨ë¸ì˜ ì¶œë ¥ ê°’ì„ ì›í•˜ëŠ” íƒ€ê²Ÿ ë ˆì´ë¸”ì— ë§žê²Œ íŠœë‹í•˜ëŠ” ê²ƒì´ LoRAì˜ í•µì‹¬ ê°œë…ì´ë¼ê³  í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ì½”ë“œìƒìœ¼ë¡œëŠ” ì•„ëž˜ì™€ ê°™ì´ êµ¬í˜„í•  ìˆ˜ ìžˆëŠ”ë°ìš”. ê¸°ì¡´ì— ëª¨ë¸ì—ì„œ ì‚¬ìš©í•˜ë˜ `Linear Layer`ë¥¼ LoRAì˜ ë¡œì§ì´ ì ìš©ëœ ì»¤ìŠ¤í…€ í´ëž˜ìŠ¤ë¡œ êµì²´í•´ì£¼ë©´ ì ìš©í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
`if self.r > 0:` ë¼ëŠ” if ë¬¸ì´ ì¶”ê°€ëœ ë¶€ë¶„ì´ LoRAê°€ ì ìš©ëœ ë¶€ë¶„ìž…ë‹ˆë‹¤.

```python

class Linear(nn.Linear, LoRALayer):
    # LoRA implemented in a dense layer
    def __init__(
        self, 
        in_features: int, 
        out_features: int, 
        r: int = 0, 
        lora_alpha: int = 1, 
        lora_dropout: float = 0.,
        fan_in_fan_out: bool = False, # Set this to True if the layer to replace stores weight like (fan_in, fan_out)
        merge_weights: bool = True,
        **kwargs
    ):
        nn.Linear.__init__(self, in_features, out_features, **kwargs)
        LoRALayer.__init__(self, r=r, lora_alpha=lora_alpha, lora_dropout=lora_dropout,
                           merge_weights=merge_weights)

        self.fan_in_fan_out = fan_in_fan_out
        # Actual trainable parameters
        if r > 0:
            self.lora_A = nn.Parameter(self.weight.new_zeros((r, in_features)))
            self.lora_B = nn.Parameter(self.weight.new_zeros((out_features, r)))
            self.scaling = self.lora_alpha / self.r
            # Freezing the pre-trained weight matrix
            self.weight.requires_grad = False
        self.reset_parameters()
        if fan_in_fan_out:
            self.weight.data = self.weight.data.T

    def reset_parameters(self):
        nn.Linear.reset_parameters(self)
        if hasattr(self, 'lora_A'):
            # initialize A the same way as the default for nn.Linear and B to zero
            nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))
            nn.init.zeros_(self.lora_B)

    def train(self, mode: bool = True):
        def T(w):
            return w.T if self.fan_in_fan_out else w
        nn.Linear.train(self, mode)
        if self.merge_weights and self.merged:
            # Make sure that the weights are not merged
            if self.r > 0:
                self.weight.data -= T(self.lora_B @ self.lora_A) * self.scaling
            self.merged = False
    
    def eval(self):
        def T(w):
            return w.T if self.fan_in_fan_out else w
        nn.Linear.eval(self)
        if self.merge_weights and not self.merged:
            # Merge the weights and mark it
            if self.r > 0:
                self.weight.data += T(self.lora_B @ self.lora_A) * self.scaling
            self.merged = True

    def forward(self, x: torch.Tensor):
        def T(w):
            return w.T if self.fan_in_fan_out else w
        if self.r > 0 and not self.merged:
            result = F.linear(x, T(self.weight), bias=self.bias)
            if self.r > 0:
                result += (self.lora_dropout(x) @ self.lora_A.T @ self.lora_B.T) * self.scaling
            return result
        else:
            return F.linear(x, T(self.weight), bias=self.bias)
```

ìœ„ ì½”ë“œì—ì„œ ì£¼ëª©í•  ë¶€ë¶„ì€ `eval` í•¨ìˆ˜ìª½ì¸ë°ìš”. 

```python
def eval(self):
    def T(w):
        return w.T if self.fan_in_fan_out else w
    nn.Linear.eval(self)
    if self.merge_weights and not self.merged:
        # Merge the weights and mark it
        if self.r > 0:
            self.weight.data += T(self.lora_B @ self.lora_A) * self.scaling # í–‰ë ¬ì„ í•©ì¹˜ëŠ” ë¶€ë¶„
        self.merged = True
```

LoRAê°€ í–‰ë ¬ ì—°ì‚°ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ê¸° ë•Œë¬¸ì— ê¸°ì¡´ í–‰ë ¬ $W_0$ë¥¼ LoRAì—ì„œ ì‚¬ìš©í•˜ëŠ” $A$, $B$ í–‰ë ¬ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì´ ìž¬êµ¬ì„±í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

$$W = W_0 + BA$$

ì´ë ‡ê²Œ í•¨ìœ¼ë¡œì¨ ì–»ì„ ìˆ˜ ìžˆëŠ” ì´ì ì€ ìƒˆë¡­ê²Œ í•™ìŠµí•œ íŒŒë¼ë¯¸í„°ë¥¼ ê¸°ì¡´ì— í•™ìŠµëœ pretrained modelì— í•©ì³ì¤Œìœ¼ë¡œì¨ ì¶”ê°€ì ì¸ ì—°ì‚°ì´ í•„ìš”í•˜ì§€ ì•Šê²Œ ë˜ì–´ ì†ë„ë„ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë©´ì„œ ì•„í‚¤í…ì³ì˜ ë³€ê²½ë„ í•„ìš”ì—†ì–´ì§€ê²Œ ë©ë‹ˆë‹¤.

### Alpaca-LoRA
ìµœê·¼ì— ìœ í–‰í•˜ëŠ” LLaMAì˜ ë³€í˜•ì¸ Alpacaì—ë„ LoRAê°€ ì ìš©ëœ ì˜¤í”ˆì†ŒìŠ¤ í”„ë¡œì íŠ¸ë“¤ì´ ê³µê°œë˜ê³  ìžˆëŠ”ë°ìš”. [Huggingfaceì—ì„œ ê³µê°œí•œ PEFT ë¼ì´ë¸ŒëŸ¬ë¦¬](https://github.com/huggingface/peft)ë¥¼ ì´ìš©í•˜ë©´ ì•„ëž˜ì™€ ê°™ì´ ì ìš©ë„ ë§¤ìš° ê°„ë‹¨í•˜ê²Œ í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

```python

from peft import (
    LoraConfig,
    get_peft_model,
    get_peft_model_state_dict,
    prepare_model_for_int8_training,
    set_peft_model_state_dict,
)
from transformers import LlamaForCausalLM, LlamaTokenizer

def train(...):
    model = prepare_model_for_int8_training(model)
    config = LoraConfig(
        r=lora_r,
        lora_alpha=lora_alpha,
        target_modules=lora_target_modules,
        lora_dropout=lora_dropout,
        bias="none",
        task_type="CAUSAL_LM",
    )
    model = get_peft_model(model, config)
```

## IA3
IA3(**I**nfused **A**dapter by **I**nhibiting and **A**mplifying **I**nner **A**ctivations)ì˜ ê°œë…ì„ ê°„ë‹¨í•˜ê²Œ ì„¤ëª…í•˜ìžë©´, LoRAì™€ ë¹„ìŠ·í•œ ë°©ë²•ìœ¼ë¡œ ì ì€ íŒŒë¼ë¯¸í„°ë§Œì„ ì¶”ê°€í•´ì„œ ëª¨ë¸ì„ íŠœë‹í•  ìˆ˜ ìžˆëŠ” ë°©ë²•ë¡  ìž…ë‹ˆë‹¤. ì´ë¦„ì—ë„ ë‚˜ì™€ìžˆë“¯ì´ ë‰´ëŸ´ë„¤íŠ¸ì›Œí¬ì˜ Inner Activationì„ ì¤„ì´ê¸°ë„í•˜ê³  ëŠ˜ë¦¬ê¸°ë„í•˜ëŠ” ì–´ëŒ‘í„°ë¥¼ ì¤‘ê°„ì— ì‚½ìž…í•˜ëŠ” ë°©ë²•ë¡ ì¸ë°ìš”. LoRAì˜ ê²½ìš°ì—ëŠ” hidden stateì— ìƒˆë¡œìš´ ê°’ì„ ë”í•´ì£¼ëŠ” ê¸°ë²•ì´ì—ˆë‹¤ë©´ IA3ì˜ ê²½ìš°ì—ëŠ” `Self-Attention, Cross-Attentionì—ì„œì˜ Key, Value ê°’ì„ rescaleí•´ì£¼ëŠ” ë²¡í„°ì™€ position-wise feed-forward networkì˜ ê°’ì— rescaleì„ í•´ì£¼ëŠ” ë²¡í„°ë¥¼ ì¶”ê°€í•´ì„œ ëª¨ë¸ì„ íŠœë‹`í•´ì£¼ëŠ” ê¸°ë²•ìž…ë‹ˆë‹¤.

![IA3](https://user-images.githubusercontent.com/7252598/230011306-0e6184ac-af46-4920-b483-03e2481d0457.png)

IA3(T-Fewë¼ê³ ë„ ë¶€ë¦„)ëŠ” ê¸°ì¡´ì— ê³µê°œëœ LoRAë³´ë‹¤ ì ì€ íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ë©´ì„œ ë†’ì€ ì„±ëŠ¥ì„ ë‚´ëŠ” ê²ƒìœ¼ë¡œ ì•Œë ¤ì ¸ìžˆìœ¼ë©°, GPT-3ë¥¼ in-context learning í–ˆì„ë•Œ ë³´ë‹¤ë„ ì„±ëŠ¥ì´ ì¢‹ë‹¤ ë¼ê³  ì£¼ìž¥í•˜ê³  ìžˆìŠµë‹ˆë‹¤.

PEFT ì„±ëŠ¥ë¹„êµ ê·¸ëž˜í”„ | ICL ì„±ëŠ¥ë¹„êµí‘œ
---|---
![ia3_performance](https://user-images.githubusercontent.com/7252598/230011771-6be13446-6fd8-4a6d-a558-219cdd975eee.png) | ![image](https://user-images.githubusercontent.com/7252598/232739247-1ee59fc6-7141-49ca-9500-655d03029580.png)

IA3ë„ LoRAì™€ ë§ˆì°¬ê°€ì§€ë¡œ Linear Layerë¥¼ ì»¤ìŠ¤í…€ êµ¬í˜„ì²´ë¡œ ë³€ê²½í•¨ìœ¼ë¡œì¨ êµ¬í˜„ì´ ê°€ëŠ¥í•˜ë©°, LoRA Layerì— ëŒ€í•œ configurationì„ ìˆ˜ì •í•´ì„œ êµ¬í˜„í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ë‹¤ìŒì€ IA3ì— ëŒ€í•œ êµ¬í˜„ì²´ìž…ë‹ˆë‹¤.

- `configs/ia3.json`
```json
{
    "lora_scaling_rank": 1,
    "lora_rank": 0,
    "lora_init_scale": 0.0,
    "lora_modules": ".*SelfAttention|.*EncDecAttention|.*DenseReluDense",
    "lora_layers": "k|v|wi_1.*",
    "trainable_param_names": ".*lora_b.*",
    "model_modifier": "lora",
    "lr": 3e-3,
    "num_steps": 1000
}
```

- [LoRAê¸°ë°˜ IA3 êµ¬í˜„ì²´](https://github.com/r-three/t-few/blob/4e581fa0b8f53e36da252a15bd581d365d4dd333/src/models/lora.py#L7)

```python

def modify_with_lora(transformer, config):
    for m_name, module in dict(transformer.named_modules()).items():
        if re.fullmatch(config.lora_modules, m_name):
            for c_name, layer in dict(module.named_children()).items():
                if re.fullmatch(config.lora_layers, c_name):
                    assert isinstance(
                        layer, nn.Linear
                    ), f"LoRA can only be applied to torch.nn.Linear, but {layer} is {type(layer)}."
                    setattr(
                        module,
                        c_name,
                        LoRALinear(layer, config.lora_rank, config.lora_scaling_rank, config.lora_init_scale),
                    )
    return transformer

class LoRALinear(nn.Module):
    def __init__(self, linear_layer, rank, scaling_rank, init_scale):
        super().__init__()
        self.in_features = linear_layer.in_features
        self.out_features = linear_layer.out_features
        self.rank = rank
        self.scaling_rank = scaling_rank
        self.weight = linear_layer.weight
        self.bias = linear_layer.bias
        if self.rank > 0:
            self.lora_a = nn.Parameter(torch.randn(rank, linear_layer.in_features) * init_scale)
            if init_scale < 0:
                self.lora_b = nn.Parameter(torch.randn(linear_layer.out_features, rank) * init_scale)
            else:
                self.lora_b = nn.Parameter(torch.zeros(linear_layer.out_features, rank))
        if self.scaling_rank:
            self.multi_lora_a = nn.Parameter(
                torch.ones(self.scaling_rank, linear_layer.in_features)
                + torch.randn(self.scaling_rank, linear_layer.in_features) * init_scale
            )
            if init_scale < 0:
                self.multi_lora_b = nn.Parameter(
                    torch.ones(linear_layer.out_features, self.scaling_rank)
                    + torch.randn(linear_layer.out_features, self.scaling_rank) * init_scale
                )
            else:
                self.multi_lora_b = nn.Parameter(torch.ones(linear_layer.out_features, self.scaling_rank))

    def forward(self, input):
        if self.scaling_rank == 1 and self.rank == 0:
            # parsimonious implementation for ia3 and lora scaling
            if self.multi_lora_a.requires_grad:
                # ì´ ë¶€ë¶„ì—ì„œ IA3ì˜ ì—°ì‚°ì´ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤.
                hidden = F.linear((input * self.multi_lora_a.flatten()), self.weight, self.bias)
            else:
                hidden = F.linear(input, self.weight, self.bias)
            if self.multi_lora_b.requires_grad:
                hidden = hidden * self.multi_lora_b.flatten()
            return hidden
        else:
            # general implementation for lora (adding and scaling)
            weight = self.weight
            if self.scaling_rank:
                weight = weight * torch.matmul(self.multi_lora_b, self.multi_lora_a) / self.scaling_rank
            if self.rank:
                weight = weight + torch.matmul(self.lora_b, self.lora_a) / self.rank
            return F.linear(input, weight, self.bias)
```

ì „ë°˜ì ìœ¼ë¡œ LoRAì˜ êµ¬í˜„ì•ˆì—ì„œ ë³€í˜•ëœ í˜•íƒœìž„ì„ í™•ì¸í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ì•„ë§ˆ ì–¸ì  ê°€ Huggingfaceì˜ PEFT ë¼ì´ë¸ŒëŸ¬ë¦¬ ì•ˆì—ë„ ë“¤ì–´ê°ˆ ë‚ ì´ ì˜¤ì§€ ì•Šì„ê¹Œ ì‹¶ë„¤ìš”.
# ë§ˆì¹˜ë©°
ìµœê·¼ ëª‡ë…„ê°„ [Scaling laws](https://arxiv.org/abs/2001.08361)ì— ë”°ë¼ ì–¸ì–´ëª¨ë¸ì˜ í¬ê¸°ê°€ ì ì  ì»¤ì§€ê³  ìžˆëŠ” ì‹œëŒ€ì— ì‚´ê³  ìžˆëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ìƒí™© ì†ì—ì„œ ì¸í”„ë¼ê°€ ì—†ëŠ” ê°œì¸ì´ë‚˜ ê¸°ì—…ì—ê²ŒëŠ” ì ì€ ìžì›ìœ¼ë¡œë„ ëª¨ë¸ì„ ë¹ ë¥´ê²Œ íŠœë‹í•  ìˆ˜ ìžˆëŠ” PEFT(Parameter Efficient Fine-Tuning)ê°€ í›Œë¥­í•œ ëŒ€ì•ˆì´ ë  ìˆ˜ ìžˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ì˜¤ëŠ˜ì€ PEFTì—ì„œ ê°€ìž¥ ìœ ëª…í•œ ë°©ë²•ë¡ ì¤‘ í•˜ë‚˜ì¸ LoRAì™€ IA3ë¼ëŠ” ê°œì„ ëœ ë°©ë²•ë¡ ì„ ë‹¤ë£¨ì–´ë´¤ëŠ”ë°ìš”. LLaMA, [Alpaca](https://devocean.sk.com/search/techBoardDetail.do?ID=164659)ì˜ ë“±ìž¥ê³¼ í•¨ê»˜ Quantizationê³¼ PEFTë¥¼ í™œìš©í•´ì„œ ë¹…ëª¨ë¸ì„ ê°œì¸ì´ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìžˆê²Œ í•˜ëŠ” ì‹œëŒ€ê°€ ì ì  ì˜¤ê³  ìžˆëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤. ì ì€ ë¹„ìš©ìœ¼ë¡œë„ ë†’ì€ ì„±ëŠ¥ì„ ê°–ëŠ” ë‚˜ë§Œì˜ íŠœë‹ëœ ëª¨ë¸ì„ ê°–ê³  ì‹¶ë‹¤ë©´ ì˜¤ëŠ˜ ì†Œê°œí•´ë“œë¦° PEFTì— ëŒ€í•´ì„œ ê³ ë ¤í•´ë³´ì‹œëŠ” ê±´ ì–´ë–¨ê¹Œìš”?ðŸ™‚

## ì°¸ê³ ìžë£Œ
- [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)
- [Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning](https://arxiv.org/abs/2012.13255)
- [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
- [LoRA Github](https://github.com/microsoft/LoRA)
- [Alpaca-LoRA Github](https://github.com/tloen/alpaca-lora/blob/8bb8579e403dc78e37fe81ffbb253c413007323f/finetune.py#L176)
- [PEFT Library Huggingface](https://github.com/huggingface/peft)
- [IA3 NVIDIA NeMo](https://github.com/NVIDIA/NeMo/commit/cb2793c0c7bb352e1dfd8c349a96efc1dd260179)
- [Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning](https://arxiv.org/abs/2205.05638)
- [IA3(T-Few) Github](https://github.com/r-three/t-few)
- [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361)
- [ChatGPT, LLaMA ê·¸ë¦¬ê³  ì´ì   Alpaca?](https://devocean.sk.com/search/techBoardDetail.do?ID=164659)
