# Transformer 사용법
이번장에서는 high-level의 시각에서 트랜스포머 모델들의 구조에 대해서 살펴보겠습니다.

## 트랜스포머의 역사
다음은 트랜스포머 모델들의 역사를 한장으로 요약한 사진입니다.
![](https://huggingface.co/course/static/chapter1/transformers_chrono.png)

[Transformer 구조](https://arxiv.org/abs/1706.03762)는 2017년 6월에 발표되었습니다. 본래의 연구는 번역태스크에 초점을 맞췄었습니다. 그 후 트랜스포머 구조에 영향을 받은 여러 모델들이 발표되었고 다음과 같습니다.

- 2018년 6월: [GPT](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf), 트랜스포머 계열중에선 첫번째로 발표된 pretrained LM입니다. 트랜스포머의 디코더 부분을 활용했고, 다양한 NLP 태스크에 대해서 파인튜닝한 결과에 대해서 SOTA 성능을 기록했습니다.
- 2018년 10월: [BERT](https://arxiv.org/abs/1810.04805), 또 다른 large pretrained LM로 트랜스포머의 인코더 부분만 활용했으며, 문장들에 대해서 더 좋은 representation을 생성하기 위해 설계되었습니다. (가장 많이 쓰이는 모델중 하나이며, 다음장에서 더 자세히 다룰 예정입니다)
- 2019년 2월: [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), 개선된 버전의 GPT 모델로써 윤리적 이슈로 인해 바로 공개되진 않았었습니다.
- 2019년 10월: [DistilBERT](https://arxiv.org/abs/1910.01108), 버트의 경량화된 버전으로 60% 빠른 속도와 40%의 메모리 절약하면서도 97%의 성능을 유지했습니다. 이때부터 대형 언어모델의 경량화 연구가 활발해졌습니다.
- 2019년 10월: [BART](https://arxiv.org/abs/1910.13461) & [T5](https://arxiv.org/abs/1910.10683), 트랜스포머의 인코더 디코더 구조를 활용한 pretrained LM모델입니다.
- 2020년 5월: [GPT-3](https://arxiv.org/abs/2005.14165), 기존 GPT-2 모델보다 훨씬 큰 버전의 pretrained LM입니다. 파인튜닝 없이(zero-shot learning) 다양한 NLP 태스크를 잘 수행할 수 있는 것으로 알려졌습니다.

```{note}
- GPT 계열 (auto-regressive)
- BERT 계열 (auto-encoding)
- BART/T5 계열 (sequence-to-sequence)
```

## 트랜스포머 게열의 모델들은 언어모델입니다
GPT, BERT, BART, T5 등등 위에서 언급된 모든 트랜스포머 모델들은 언어모델(Language Model)로 학습되었습니다. 대량의 말뭉치에 대해 self-supervised 방법으로 학습했습니다. self-supervised learning은 학습방법 중 하나로써 모델에 입력으로 들어오는 표현 간의 특정한 관계를 배우는 것을 목표로 학습합니다. 이는 label 데이터를 따로 사용하지 않아도 됨을 뜻합니다. (예컨데, 문장 토큰의 일부를 마스킹한 뒤 해당 토큰이 무엇인지 맞추거나, 혹은 연속된 두 문장이 연관이 있는지를 맞추는 문제등을 들 수 있습니다)

이러한 모델은 통계적으로 언어에 대한 이해를 할 수 있지만 특정 태스크에 대해서는 유용하지 못할 것입니다. 이러한 이유로 보통의 사전학습 모델은 각 태스크에 맞는 `transfer learning`을 수행하게 됩니다. 이러한 과정에서 모델은 supervised 방법(human-annotated labels를 사용한)으로 파인튜닝됩니다.

언어모델은 크게 두가지 방법으로 학습됩니다. 
첫번째 방법은 문장 내에서 `n`개의 단어들을 읽은 뒤 다음 단어가 무엇인지 맞추는 방법입니다. 이러한 방법을 `casual language modeling` 이라고 부릅니다. 왜냐하면 출력값이 과거에서부터 현재까지의 입력에 영향을 받기 때문입니다. (미래에 등장할 값은 고려하지 않습니다)

(casual Figure)

두번째 방법은 `masked language modeling`입니다. 문장 내에서 마스킹된 단어가 무엇인지 예측하는 방법입니다.

(mlm figure)


## 트랜스포머계열의 모델들은 빅모델입니다
몇몇의 DistilBERT 같은 아웃라이어를 제외하면 더 나은 성능을 위한 보통의 전략은 모델의 크기와 학습데이터를 늘리는 것입니다.
(modelsize figure)

안타깝게도 큰 모델을 학습할수록 많은 양의 데이터가 필요하게되고 이는 시간 및 컴퓨팅 자원의 비용이 높다고 할 수 있습니다. 이를 환경 문제로 해석할땐 다음과 같은 결과를 볼 수 있습니다.
(costgraph figure)

이러한 이유로 언어모델을 모두가 처음부터 학습하기보다는 널리 공유하고, 함께 사용하는 것이 전체적인 학습비용 및 환경문제에 있어 매우 중요하다고 할 수 있습니다.

## 전이학습 (Transfer Learning)
사전학습(`Pretraining`)은 모델을 처음부터 학습하는 행위입니다. 랜덤하게 초기화된 가중치로 사전지식 없이 모델을 처음부터 학습하는 것을 의미합니다.
![ptr](https://huggingface.co/course/static/chapter1/pretraining.png)
보통 사전학습은 대량의 데이터에 대해 이루어지고, 학습하는데 몇주의 시간이 걸리기도 합니다.

반면, 파인튜닝(`Fine-tuning`)의 경우 사전학습 이후에 진행하게 되는 학습을 의미합니다. 파인튜닝을 하기 위해선 먼저 사전학습된 언어모델이 필요하고, 우리가 하고자하는 특정 태스크에 대한 추가 데이터가 필요합니다.

잠깐, 그렇다면 간단하게 직접 태스크에 대해서 학습하면 되지 않을까요? 왜 굳이 사전학습 모델을 만들고 거기에 파인튜닝을 해야할까요? 몇가지 이유가 있습니다.

- 사전하습모델은 이미 파인튜닝에 사용하는 데이터셋과 어느정도 유사한 데이터셋에 학습이 된 상태입니다. 사전학습을 하면서 모델은 언어에 대한 일종의 이해능력을 얻게 됩니다. 그러므로 파인튜닝 과정은 사전학습을 하는 동안 초기 모델에서 얻었던 지식에 대한 이점을 활용한다고 볼 수 있습니다
- 사전학습모델은 이미 많은 양의 데이터를 학습한 상태이므로 파인튜닝은 약간의 데이터만으로도 가능합니다
- 좋은 결과를 얻기 위한 시간과 자원적 비용이 낮습니다

예를 들면, 영어로 사전학습한 언어모델을 arXiv(논문) 문서에 파인튜닝하면 과학/연구 기반의 모델을 만들 수 있습니다. 사전학습 모델이 갖고 있던 지식도 전이(`transferred`)되는 것이죠. 이러한 이유로 전이학습(`transfer learning`) 이라는 표현을 사용합니다.
(transfer learning figure)

그러므로, 모델을 파인튜닝하는 것은 적은 시간, 뎅터, 재정, 환경비용등의 이점을 갖습니다. 뿐만 아니라 사전학습 대비 더 쉽고 빠르게 다른 파인튜닝 과정들을 적용할 수 있습니다.

이러한 과정은 정맒 많은 데이터를 갖고 있는게 아닌이상 처음부터 학습하는 것보다 좋은 결과를 내기 때문에 사전하습 모델을 활용하는 것이 좋습니다.


## 일반적인 모델 구조
이번 섹션에서는 트랜스포머 모델의 일반적인 구조에 대해 다룰 예정입니다. 몇몇의 개념을 이해하지 못한다고해서 걱정하실 필요는 없습니다. 추후에 각 컴포넌트별로 더 디테일한 내용을 다룰 예정입니다.


### 들어가며
모델은 크게 두가지 블록으로 이루어져있습니다.
- **인코더(Encoder)**: 인코더에서는 입력되는 값을 받아서 이에 대한 표현(representation)을 생성합니다. 피쳐를 만든다고 생각하시면 되는데요. 입력을 모델이 이해하기 좋은 형태로 최적화하는 과정이라고 생각할 수 있습니다.
- **디코더(Decoder)**: 디코더에서는 인코더가 만든 입력에 대한 표현(피쳐)을 사용해서 타겟 시퀀스를 생성합니다. 출력을 생성하기 위해 최적화하는 과정이라고 볼 수 있습니다.
![](https://huggingface.co/course/static/chapter1/transformers_blocks.png)

인코더와 디코더는 태스크에 따라 독립적으로 사용 할 수 있습니다.

- **인코더 전용 모델(*Encoder-only models*)**: 문장 분류 및 개체명인식과 같이 입력에 대한 이해가 필요한 작업에 적합합니다
- **디코더 전용 모델(*Decoder-only models*)**: 텍스트 생성과 같은 작업에 적합합니다
- **인코더-디코더 모델(*Encoder-decoder models* or *sequence-to-sequence models*)**: 번역 또는 요약과 같이 입력이 필요한 작업에 적합합니다

### 어텐션 레이어 (Attention layers)
트랜스포머 모델의 주요 기능은 어텐션 레이어라는 특수 레이어로 구축됩니다. 사실 트랜스포머 모델을 소개하는 논문의 제목이 바로 "**Attention is All You Need**"입니다. 어텐션 레이어에 대한 내용은 뒤에서 더 자세히 다루도록 하겠습니다. 지금은 이 레이어가 각 단어의 표현을 처리할 때 입력한 문장의 특정 단어를 다른 단어에 비해 상대적으로 더 집중해서 보는 방법으로 언어를 이해한다고 생각하시면 됩니다.

이것을 상황에 맞게 설명하려면 텍스트를 영어에서 한국어로 번역하는 작업을 고려해볼 수 있습니다. "You like this course" 입력이 주어지면 번역 모델은 "like"라는 단어에 대한 적절한 번역을 얻기 위해 인접 단어 "You"에도 주의를 기울여야 합니다. 왜냐하면 한국어에서 동사 "like"는 다음에 따라 다르게 활용되기 때문입니다. 더 복잡한 문장(및 더 복잡한 문법 규칙)을 사용하면 동일한 개념이 자연어와 관련된 모든 작업에 적용됩니다. 단어 자체에 의미가 있지만 그 의미는 문맥에 의해 크게 영향을 받습니다.

어텐션 레이어가 무엇인지 이해했으므로 이제 Transformer 아키텍쳐를 자세히 살펴보겠습니다.

### 본래 Transformer 모델 구조
Transformer 아키텍쳐는 원래 번역용으로 설계되었습니다. 학습 중에 인코더는 특정 언어로 입력(문장)을 받고 디코더는 원하는 타겟 언어로 같은 의미의 문장을 입력받습니다. 인코더에서 어텐션 레이어는 문장의 모든 단어를 보며 컨텍스트를 분석할 수 있습니다. 그러나 디코더는 현재시점에서 출력된 토큰을 보면서 다음 토큰을 예측하기 때문에 순차적으로 작동하며 이미 번역된 문장의 단어에만 주의를 기울일 수 있습니다. 예를 들어 번역된 대상의 처음 세 단어를 예측했고 그 다음인 네번째 단어를 예측하려는 경우 디코더에서는 처음 세 단어를 입력으로 사용하고 다음 인코더에서는 번역하려는 언어의 모든 입력을 사용하여 네 번째 단어를 예측하려고 시도합니다.

훈련 중 속도를 높이기 위해 디코더의 입력으로 전체 토큰을 제공하지만 미래 단어를 사용할 수 없도록 어텐션을 마스킹처리합니다. 예를 들어, 네 번째 단어를 예측하려고 할 때 어텐션은 토큰 1에서 3까지의 값만 활용할 수 있습니다.

원래 Transformer 아키텍쳐는 왼쪽에 인코더가 있고 오른쪽에 디코더가 있는 다음과 같습니다.
![transformer(]https://huggingface.co/course/static/chapter1/transformers.png)

디코더 블록의 첫 번째 어텐션 레이어는 디코더에 입력된 타겟 언어의 모든 (과거) 입력에 주의를 기울이지만 두 번째 어텐션레이어는 인코더 입력된 원래 언어의 출력을 사용합니다. 따라서 디코더에서는 현재 번역될 단어를 가장 잘 예측하기 위해 원래 언어의 전체 입력 문장에 접근 할 수 있습니다. 이것은 번역할 언어가 단어를 다른 순서로 배치하는 문법 규칙을 따른다던지 혹은 문장의 뒷부분에 제공된 일부 컨텍스트가 번역할 단어의 최상의 번역을 결정하는 데 도움이 될 수 있기 때문에 매우 유용합니다.

어텐션 마스크는 인코더/디코더에서 모두 사용되고, 모델이 학습할때 집중하면 안되는 토큰들에 집중하지 않도록 막아주는 역할을 합니다. 예를 들어 학습할때 배치단위의 길이를 맞춰주기 위해 사용하는 특수토큰인 패딩토큰(`<pad>`)이라던지, 디코더에서 아직 예측하지 않은  단어를 보지 못하게 하는데 사용할 수 있습니다.

### Architectures vs checkpoints
이번챕터에서 트랜스포머 계열의 모델들을 정리해보았습니다. 앞으로 아키텍쳐, 체크포인트, 모델등의 여러 표현들이 나올텐데요. 간단하게 용어 정리를 해보겠습니다.

- **아키텍쳐(Architecture)**: 모델의 뼈대, 구조를 의미합니다. 모델에서 사용되는 각 레이어에 대한 정의 혹은 모델 내에서 사용되는 연산을 의미합니다
- **체크포인트(Checkpoint)**: 주어진 아키텍쳐에서 로딩되는 모델의 가중치를 의미합니다
- **모델(Model)**: 모델은 아키텍쳐나 체크포인트 만큼 정확하게 사용되진 않는 포괄적인 용어입니다. 아키텍쳐와 체크포인트 둘다를 의미할 수도 있습니다. 

예를 들어, BERT는 아키텍쳐이고, `bert-base-cased`는 Google팀에서 학습한 가중치의 집합, 체크포인트입니다. 하지만 여전히 "BERT 모델" 혹은 "bert-base-cased 모델"이라고도 부를 수도 있습니다.
