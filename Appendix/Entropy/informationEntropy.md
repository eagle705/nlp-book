# 정보와 엔트로피
이번에는 머신러닝에서 가장 많이 쓰이는 Entropy의 개념에 대해서 배워보도록 하겠습니다. Entropy의 개념을 이해하기 위해선 정보량에 대한 개념도 알아야하는데요. 지금부터 한번 알아보도록 하겠습니다.

## 정보 (Information)
머신러닝에서 `정보`란 `놀라움의 정도`라고 정의해볼 수 있습니다.  
예컨데, `놀라움이 크다 == 정보가 많다`, `놀라움이 작다 == 받은 정보가 적다`로 말할 수 있는데요.  

여기서 말하는 `놀라움의 정도`란 **어떤 일이 발생할 확률과 관련**이 있습니다.   
ex 1) `발생 확률이 적은 일이 일어날수록 놀라움이 큼`  
ex 2) 예상 문제 중 1문제만을 풀었는데, 시험에서 100점 맞는 사건 -> 발생할 확률이 적기 때문에 '놀라움'이 큼


정리하면, `정보란 발생확률과 반비례 관계`이며 `일어날 확률이 낮은 일이 일어났을때 굉장히 놀라운일이며 이때 정보가 높다` 라고 할 수 있겠습니다.

$$
\text { 정보 } \propto \frac{1}{\text { 발생 확률 }}
$$

### 정보의 단위
정보이론에서 정보의 단위는 주로 `bit`를 사용합니다. 흔한 일이라면 놀랍지 않은일이고, 정보량이 적으므로 적은 수의 bit로 표현이 가능합니다. 반대로 발생확률이 낮은 놀라운 일이라면 정보량이 높으므로 많은 수의 bit가 필요합니다.  
정보는 다음과 같이 정의할 수 있습니다.

확률변수 $X$ 가 $x$ 값이라는 정보를 알았을 때 얻은 정보량 

$$
\operatorname{Information}(X=x):=\log _2 \frac{1}{P(X=x)}=-\log _2 P(X=x)
$$

정보와 확률은 반비례, 즉 역수 관계이므로 $\log _2$를 사용하면 마이너스(-)를 앞으로 빼내면서 $\operatorname{Information}(X=x):=-\log _2 P(X=x)$로 확률에 로그를 씌운 후 마이너스를 곱한 값으로 표현할 수 있습니다.

이때 $\log _2$는 왜 쓰였을까요? 

### $\log _2$의 배경
길이 $N$의 0, 1로 이루어진 이진 문자열 로또를 Uniform하게 뽑는다고 가정해보겠습니다. 이때 정보량은 총 $N$ bit일 것입니다. $N$ bit 문자열 중 우리가 뽑은 문자열이 나올 확률은 얼마일까요? $\frac{1}{2^N}$로 계산할 수 있을 것 입니다. 

이때 놀라움의 정도를 말로 표현해본다면 "${2^N}$개 중에 우리가 뽑은 번호가 나오다니!" 정도로 표현할 수 있을 것 같네요. 

발생 확률이 $\frac{1}{2^N}$인 사건에 대해서 그 사건이 일어나려면 결국 필요한 정보량은 $N$ bit라고 할 수 있을 것 입니다.

이 관점에서 정보량 정의의 식은 다음과 같습니다.

$$
N \text { bits }=-\log _2 \frac{1}{2^N}
$$

위와 같은 계산과정을 거치면서 정보량을 표현할 때 $\log _2$를 관행적으로 표현하게 되었습니다.

## 엔트로피 (Entropy)
엔트로피는 일반적으로 **확률 변수 $X$에 대해서 $X$를 알아냄으로써 얻을 수 있는 정보의 기대값**을 의미합니다.
정보의 기대값이므로 정보를 나타내는 $-\log _2 P(X=x)$에 그 정보에 대한 사건이 나타날 수 있는 확률 $p(X)$를 곱해주는 형태로 `기대값`을 아래와 같이 계산할 수 있습니다.

$$
H(X):=\mathbb{E}_{x \sim p(X)}[-\log p(x)]=-\mathbb{E}_{x \sim p(X)}[\log p(x)]
$$
- 이산 확률 변수
$$
H(X)=-\sum_i p_X\left(x_i\right) \log p_X\left(x_i\right)
$$
- 연속 확률 변수
$$
H(X)=-\int_{-\infty}^{\infty} p(x) \log p(x) d x
$$

### Entropy의 특징
첫번째 특징은 `Entropy(엔트로피)는 항상 음이 아닌 실수의 값`을 가집니다. 엔트로피는 위에서 말한것 처럼 정보(확률의 역수에 로그를 취한 것)에 대한 기대값이기 때문에 늘 음이 아닌 실수가 나오게 됩니다.

$$H(X)\ge0$$

두번째 특징은 `엔트로피는 Uniform distribution일 때 최대`가 됩니다. 해석해보자면, 모든 경우 동일한 확률로 나오기 때문에 무엇이 나올지 알 수 없으므로 가장 많은 정보를 필요로하게 됩니다.  
정보는 놀라움의 정도라 했는데요. 아래 그림을 보면, 특정 값이 나올 확률이 높다면 당연히 놀랍지 않은 일일 것이고 정보가 낮다는 뜻일겁니다. 그러므로 특정 값이 나올 확률이 높은 케이스는 엔트로피가 낮은 반면, 어떤 값이 나올지 모르는 Uniform distribution의 경우 놀라운 일이 많고, 정보이 필요하며 엔트로피가 높다고 할 수 있습니다.

![image](https://user-images.githubusercontent.com/7252598/229152411-eadc0cad-d862-4bad-abd1-682a0f08b45b.png)

- $[0.2,0.2,0.2,0.2,0.2]: \log 5=2.32$
- $[0.0,0.1,0.8,0.1,0.0]: 0.2 \log 10+0.8 \log 1.25=0.922$
- $[0.0,0.0,1.0,0.0,0.0]: 1 \log 1=0$

세번째 특징은 약간 어려울 수 있는데요. 다음과 같습니다. (이때 $X$는 $P(x)$를 따름)

$$
\begin{aligned}
& H(X) \text { 는 } \mathbb{E}_{x \sim p(x)}[-\log q(x)] \text { 의 하한이다 } \\
&H(X)=\mathbb{E}_{x \sim p(x)}[-\log p(x)] \leq \mathbb{E}_{x \sim p(x)}[-\log q(x)]
\end{aligned}
$$

이에 대한 증명은 다음과 같습니다.
$$
\begin{aligned}
& \mathbb{E}_{x \sim p(x)}[-\log p(x)]-\mathbb{E}_{x \sim p(x)}[-\log q(x)] \\
& =\left(-\sum_i p_X\left(x_i\right) \log p_X\left(x_i\right)\right)-\left(-\sum_i p_X\left(x_i\right) \log q_X\left(x_i\right)\right) \\
& =\sum_i p_X\left(x_i\right) \log \frac{q_X\left(x_i\right)}{p_X\left(x_i\right)} \leq \frac{1}{\ln 2} \sum_i p_X\left(x_i\right)\left(\frac{q_X\left(x_i\right)}{p_X\left(x_i\right)}-1\right) \\
& =\frac{1}{\ln 2} \sum_i\left(q_X\left(x_i\right)-p_X\left(x_i\right)\right)=\frac{1}{\ln 2}\left(\sum_i q_X\left(x_i\right)-\sum_i p_X\left(x_i\right)\right)=0
\end{aligned}
$$

왜 이걸 배워야할까요?  
머신러닝에서는 분포를 학습할 때 엔트로피 $H(X)$의 하한선인 $\mathbb{E}_{x \sim p(x)}[-\log q(x)]$를 최소화하는 것을 목표로 하기 때문입니다. $p(x)$를 모델링하기 어려울 경우 $q(x)$로 대신 모델링 할 일이 생기는데, $\mathbb{E}_{x \sim p(x)}[-\log q(x)]$는 $H(X)$의 하한선이므로 하한선을 최소화해서 학습을 한다면 $H(X)$ 값 또한 최소화 하는 방향으로 학습이 진행 될 수 있기 때문입니다.
