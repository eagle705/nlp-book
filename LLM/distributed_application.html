
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>PyTorch로 분산어플리케이션 개발하기 &#8212; NLP Book</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=796348d33e8b1d947c94" rel="stylesheet">
<link href="../_static/styles/bootstrap.css?digest=796348d33e8b1d947c94" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=796348d33e8b1d947c94" rel="stylesheet">

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=796348d33e8b1d947c94" rel="stylesheet">
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2">
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2">
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=4ec06e9971c5264fbd345897d5258098f11cc577" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=796348d33e8b1d947c94">
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=796348d33e8b1d947c94">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=8bf782fb4ee92b3d3646425e50f299c4e1fd152d"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-RCPD8XLR5D"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-RCPD8XLR5D');
            </script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'LLM/distributed_application';</script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="docsearch:language" content="en">
  </head>
  
  
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="180" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>

  
  <input type="checkbox" class="sidebar-toggle" name="__primary" id="__primary">
  <label class="overlay overlay-primary" for="__primary"></label>

  
  <input type="checkbox" class="sidebar-toggle" name="__secondary" id="__secondary">
  <label class="overlay overlay-secondary" for="__secondary"></label>

  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
      
<form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false">
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
    </div>
  </div>

  
  <nav class="bd-header navbar navbar-expand-lg bd-navbar" id="navbar-main"><div class="bd-header__inner bd-page-width">
  <label class="sidebar-toggle primary-toggle" for="__primary">
      <span class="fa-solid fa-bars"></span>
  </label>
  <div id="navbar-start">
    
    
  


<a class="navbar-brand logo" href="../intro.html">

  
  
  
  
  
  
  

  
    <img src="../_static/logo.png" class="logo__image only-light" alt="Logo image">
    <img src="../_static/logo.png" class="logo__image only-dark" alt="Logo image">
  
  
</a>
    
  </div>

  
  <div class="col-lg-9 navbar-header-items">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <nav class="navbar-nav">
    <p class="sidebar-header-items__title" role="heading" aria-level="1" aria-label="Site Navigation">
        Site Navigation
    </p>
    <ul id="navbar-main-elements" class="navbar-nav">
        
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../Setup/setup.html">
                        Setup
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../NLP/nlp.html">
                        Natural Language Processing
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../BasicNLP/overview/overview.html">
                        Overview
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../BasicNLP/text_representation/text_representation.html">
                        Text Representation
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../BasicNLP/dl_nlp/dl_nlp.html">
                        Deep Learning for NLP
                      </a>
                    </li>
                
            <div class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                    More
                </button>
                <div class="dropdown-menu">
                    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../BasicNLP/LM/LM.html">
                        Language Model
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../Transformer/architecture/introduction.html">
                        Introduction
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../Transformer/architecture/transformer_family.html">
                        Transformer Family
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../TransformerModels/introduction.html">
                        Transformer models
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../Exercise/introduction.html">
                        Downstream Task
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../Exercise/sentiment_analysis.html">
                        감성분석 (Sentiment Analysis)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../Exercise/Making_Transformers_Efficient_in_Production.html">
                        Making Transformers Efficient in Production
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../Exercise/Training_Transformers_from_Scratch.html">
                        Training Transformers from Scratch
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="introduction.html">
                        Introduction
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="slurm.html">
                        SLURM
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../ChatGPT/introduction.html">
                        Introduction
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../ChatGPT/LLaMA.html">
                        Meta가 쏘아올린 작은공 LLaMA
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../ChatGPT/Alpaca.html">
                        Hello, Alpaca?
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../ChatGPT/PEFT.html">
                        Parameter Efficient Fine-Tuning(PEFT)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../DevChatGPT/introduction.html">
                        Introduction
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../Appendix/appendix.html">
                        Appendix
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../Appendix/LinearAlgebra/eigenvectorvalue.html">
                        Eigenvector와 Eigenvalue
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../Appendix/LinearAlgebra/eigen_example.html">
                        Eigenvector와 Eigenvalue를 활용한 PCA 예시
                      </a>
                    </li>
                
                </div>
            </div>
            
    </ul>
</nav>
      </div>
      
    </div>

    <div id="navbar-end">
      
        <div class="navbar-end-item navbar-persistent--container">
          
<button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-toggle="tooltip">
  <i class="fa-solid fa-magnifying-glass"></i>
</button>
        </div>
      
      
      <div class="navbar-end-item">
        <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
</button>
      </div>
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
      </div>
      
    </div>
  </div>


  
  
    <div class="navbar-persistent--mobile">
<button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-toggle="tooltip">
  <i class="fa-solid fa-magnifying-glass"></i>
</button>
    </div>
  

  
  <label class="sidebar-toggle secondary-toggle" for="__secondary">
      <span class="fa-solid fa-outdent"></span>
  </label>
  

</div>
  </nav>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        
  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
      
      <div class="navbar-center-item">
        <nav class="navbar-nav">
    <p class="sidebar-header-items__title" role="heading" aria-level="1" aria-label="Site Navigation">
        Site Navigation
    </p>
    <ul id="navbar-main-elements" class="navbar-nav">
        
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../Setup/setup.html">
                        Setup
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../NLP/nlp.html">
                        Natural Language Processing
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../BasicNLP/overview/overview.html">
                        Overview
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../BasicNLP/text_representation/text_representation.html">
                        Text Representation
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../BasicNLP/dl_nlp/dl_nlp.html">
                        Deep Learning for NLP
                      </a>
                    </li>
                
            <div class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                    More
                </button>
                <div class="dropdown-menu">
                    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../BasicNLP/LM/LM.html">
                        Language Model
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../Transformer/architecture/introduction.html">
                        Introduction
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../Transformer/architecture/transformer_family.html">
                        Transformer Family
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../TransformerModels/introduction.html">
                        Transformer models
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../Exercise/introduction.html">
                        Downstream Task
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../Exercise/sentiment_analysis.html">
                        감성분석 (Sentiment Analysis)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../Exercise/Making_Transformers_Efficient_in_Production.html">
                        Making Transformers Efficient in Production
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../Exercise/Training_Transformers_from_Scratch.html">
                        Training Transformers from Scratch
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="introduction.html">
                        Introduction
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="slurm.html">
                        SLURM
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../ChatGPT/introduction.html">
                        Introduction
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../ChatGPT/LLaMA.html">
                        Meta가 쏘아올린 작은공 LLaMA
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../ChatGPT/Alpaca.html">
                        Hello, Alpaca?
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../ChatGPT/PEFT.html">
                        Parameter Efficient Fine-Tuning(PEFT)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../DevChatGPT/introduction.html">
                        Introduction
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../Appendix/appendix.html">
                        Appendix
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../Appendix/LinearAlgebra/eigenvectorvalue.html">
                        Eigenvector와 Eigenvalue
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../Appendix/LinearAlgebra/eigen_example.html">
                        Eigenvector와 Eigenvalue를 활용한 PCA 예시
                      </a>
                    </li>
                
                </div>
            </div>
            
    </ul>
</nav>
      </div>
      
      </div>
    

    
    
    <div class="sidebar-header-items__end">
      
      <div class="navbar-end-item">
        <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
</button>
      </div>
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
      </div>
      
    </div>
    
  </div>

  
  <div class="sidebar-start-items sidebar-primary__section">
    <div class="sidebar-start-items__item">
  


<a class="navbar-brand logo" href="../intro.html">

  
  
  
  
  
  
  

  
    <img src="../_static/logo.png" class="logo__image only-light" alt="Logo image">
    <img src="../_static/logo.png" class="logo__image only-dark" alt="Logo image">
  
  
</a>
    </div>
    <div class="sidebar-start-items__item">
<form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false">
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
    </div>
    <div class="sidebar-start-items__item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    들어가며
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">1장 SetUp</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../Setup/setup.html">Setup</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../Setup/setup_env.html">패키지 설치 및 환경설정</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../NLP/nlp.html">Natural Language Processing</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">2장 BasicNLP</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../BasicNLP/overview/overview.html">Overview</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../BasicNLP/overview/%ED%95%9C%EA%B8%80%EA%B3%BC%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC.html">한국어와 자연어처리</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BasicNLP/overview/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC%EC%9A%A9%EC%96%B4.html">token, sentence, vector spaces, embedding</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../BasicNLP/text_representation/text_representation.html">Text Representation</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../BasicNLP/text_representation/tf-idf.html">TF-IDF</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BasicNLP/text_representation/word2vec.html">Word2Vec &amp; GloVe</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BasicNLP/text_representation/contextual_word_representation.html">Contextual word representation</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../BasicNLP/dl_nlp/dl_nlp.html">Deep Learning for NLP</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../BasicNLP/dl_nlp/mlp.html">MLP(Multi-Layer Perceptron)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BasicNLP/dl_nlp/cnn.html">CNN</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BasicNLP/dl_nlp/rnn.html">RNN</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BasicNLP/dl_nlp/attention_mechanism.html">Attention mechanism</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../BasicNLP/LM/LM.html">Language Model</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../BasicNLP/LM/probability.html">Probability Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BasicNLP/LM/disc_gen_model.html">Discriminative VS Generative model</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">3장 Transformer</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Transformer/architecture/introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Transformer/architecture/transformer_family.html">Transformer Family</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../TransformerModels/introduction.html">Transformer models</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../TransformerModels/transformers.html">Transformers로 뭘 할 수 있을까요?</a></li>









<li class="toctree-l2"><a class="reference internal" href="../TransformerModels/transformers%EC%82%AC%EC%9A%A9%EB%B2%95.html">Transformer 사용법</a></li>
<li class="toctree-l2"><a class="reference internal" href="../TransformerModels/%EB%8B%A4%EC%96%91%ED%95%9C%EC%96%B8%EC%96%B4%EB%AA%A8%EB%8D%B8.html">언어모델의 다양한 아키텍쳐</a></li>
<li class="toctree-l2"><a class="reference internal" href="../TransformerModels/bias_and_limitations.html">언어모델의 편견 및 한계</a></li>
<li class="toctree-l2"><a class="reference internal" href="../TransformerModels/%EB%A7%BA%EC%9C%BC%EB%A9%B0.html">맺으며</a></li>
<li class="toctree-l2"><a class="reference internal" href="../TransformerModels/%ED%80%B4%EC%A6%88.html">챕터 끝 퀴즈</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">4장 Downstream Task</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Exercise/introduction.html">Downstream Task</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Exercise/sentiment_analysis.html">감성분석 (Sentiment Analysis)</a></li>









<li class="toctree-l1"><a class="reference internal" href="../Exercise/Making_Transformers_Efficient_in_Production.html">Making Transformers Efficient in Production</a></li>






<li class="toctree-l1"><a class="reference internal" href="../Exercise/Training_Transformers_from_Scratch.html">Training Transformers from Scratch</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">5장 Large Language Models(LLMs)</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="slurm.html">SLURM</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">6장 ChatGPT</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../ChatGPT/introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ChatGPT/LLaMA.html">Meta가 쏘아올린 작은공 LLaMA</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ChatGPT/Alpaca.html">Hello, Alpaca?</a></li>



<li class="toctree-l1"><a class="reference internal" href="../ChatGPT/PEFT.html">Parameter Efficient Fine-Tuning(PEFT)</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">7장 나만의 ChatGPT 만들기</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../DevChatGPT/introduction.html">Introduction</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">부록</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Appendix/appendix.html">Appendix</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Appendix/LinearAlgebra/eigenvectorvalue.html">Eigenvector와 Eigenvalue</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Appendix/LinearAlgebra/eigen_example.html">Eigenvector와 Eigenvalue를 활용한 PCA 예시</a></li>
</ul>

    </div>
</nav>
    </div>
  </div>
  

  
  <div class="sidebar-end-items sidebar-primary__section">
    <div class="sidebar-end-items__item">
    </div>
  </div>

  
  <div id="rtd-footer-container"></div>

      </div>
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

        <div class="bd-content">
          <div class="bd-article-container">
            
            <div class="bd-header-article">
                



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        <label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" data-toggle="tooltip" data-placement="right" title="Toggle primary sidebar">
            <span class="fa-solid fa-bars"></span>
        </label>
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="btn btn-sm"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<div class="dropdown dropdown-repository-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      <li><a href="https://github.com/eagle705/nlp-book" target="_blank"
   class="btn btn-sm dropdown-item"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">repository</span>
</a>
</a>
      
      <li><a href="https://github.com/eagle705/nlp-book/issues/new?title=Issue%20on%20page%20%2FLLM/distributed_application.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm dropdown-item"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">open issue</span>
</a>
</a>
      
  </ul>
</div>



<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      <li><a href="../_sources/LLM/distributed_application.md" target="_blank"
   class="btn btn-sm dropdown-item"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</a>
      
      <li>
<button onclick="printPdf(this)"
  class="btn btn-sm dropdown-item"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</a>
      
  </ul>
</div>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary" data-toggle="tooltip" data-placement="left" title="Toggle secondary sidebar">
            <span class="fa-solid fa-list"></span>
        </label>
    </div>
</div>
            </div>
            
            

<div id="jb-print-docs-body" class="onlyprint">
    <h1>PyTorch로 분산어플리케이션 개발하기</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   용어
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#node-gpu">
     Node &amp; GPU
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hw">
     HW
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   분산학습에 쓰이는 통신기술
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#mpi-message-passing-interface">
   MPI (Message Passing Interface)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#point-to-point-p2p">
     Point-to-Point(P2P)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#collective-communication">
     Collective Communication 통신(집합통신)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#d-parallelism">
   3D Parallelism
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#data-parallelism-dp">
     Data Parallelism (DP)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#dp">
       DP 구현
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-parallelism">
     Model Parallelism
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#mp">
       2가지 MP 기법
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#inter-layer">
       Inter-layer 기법의 단점
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pipeline-parallelism-pp">
     Pipeline Parallelism (PP)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#intra-layer">
       Intra-layer 기법 알아보기
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#column-parallelism">
       Column Parallelism
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#row-parallelism">
       Row Parallelism
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#mlp">
       MLP 연산
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#attention-layer">
       Attention Layer 연산
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#embedding-layer">
       Embedding Layer 연산
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gradient-accumulation">
   Gradient Accumulation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gradient-checkpointing">
   Gradient Checkpointing
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#nvidia-apex">
   NVIDIA Apex
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#reference">
   Reference
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>

            <article class="bd-article" role="main">
              
  <section class="tex2jax_ignore mathjax_ignore" id="pytorch">
<h1>PyTorch로 분산어플리케이션 개발하기<a class="headerlink" href="#pytorch" title="Permalink to this heading">#</a></h1>
<p>이번 챕터에서는 LLM 학습을 위한 여러가지 개념들에 대해서 다뤄보도록 하겠습니다.<br />
딥러닝 모델이 커지고 학습해야될 데이터양이 많아지다보면 데이터와 모델을 여러 GPU에 그리고 여러 노드에 나눠서 학습해야할 일이 생기게 됩니다. 먼저 용어부터 살펴보도록 하겠습니다.</p>
<section id="id1">
<h2>용어<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h2>
<section id="node-gpu">
<h3>Node &amp; GPU<a class="headerlink" href="#node-gpu" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Global / Local: 전체시스템 범위 / 한 노드내 범위</p></li>
<li><p>Node: 컴퓨터</p>
<ul>
<li><p>node_size: 독립된 머신의 수 (컴퓨터의 수)</p></li>
</ul>
</li>
<li><p>Rank: GPU 번호 (글로벌 프로세스 번호)</p>
<ul>
<li><p>local_rank: 해당 node에서의 프로세스 번호</p></li>
</ul>
</li>
<li><p>num_gpu: 각 노드당 사용하는 GPU 개수</p></li>
<li><p>World Size: GPU 개수 (총 글로벌 프로세스 개수이며 node_size * num_gpu)</p></li>
</ul>
<img width="1154" alt="image" src="https://github.com/eagle705/nlp-book/assets/7252598/82ea0e0c-b49c-48dc-ad10-b8c1ee06baf8"> 
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Global Rank</p></th>
<th class="head"><p>Local Rank</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><img alt="image" src="https://github.com/eagle705/nlp-book/assets/7252598/34d654d4-1cc8-44a2-b7eb-322fe55e3899" /></p></td>
<td><p><img alt="image" src="https://github.com/eagle705/nlp-book/assets/7252598/4d8bad15-d294-48fb-87d3-9d8c721c88f4" /></p></td>
</tr>
</tbody>
</table>
</section>
<section id="hw">
<h3>HW<a class="headerlink" href="#hw" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>PIO (programmed IO)</p>
<ul>
<li><p>보통 사용자가 파일을 읽고 쓰는 경우</p></li>
</ul>
</li>
<li><p>DMA (Direct Memory Access)</p>
<ul>
<li><p>바로 디스크에서 읽어오는 경우</p></li>
</ul>
</li>
<li><p>RDMA (Remote Direct Memory Access)</p>
<ul>
<li><p>네트워크 통신을 통해 다른 노드에 있는 디스크에서 DMA로 읽어오는 경우</p></li>
</ul>
</li>
</ul>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>PIO, DMA</p></th>
<th class="head"><p>RDMA</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><img width="572" alt="image" src="https://github.com/eagle705/nlp-book/assets/7252598/bad85369-cdc2-4025-93f0-62b30d713189"></p></td>
<td><p><img width="807" alt="image" src="https://github.com/eagle705/nlp-book/assets/7252598/e34aa3cc-a98c-45a8-a568-db481cc589ae"></p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="id2">
<h2>분산학습에 쓰이는 통신기술<a class="headerlink" href="#id2" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>PCI (PCIe): 한 노드 내에서의 DMA를 지원하는 버스 프로토콜</p></li>
<li><p>NVLink: 한 노드 내에서 GPU간 DMA 통신 지원 (GPU간 통신 특화라 매우 빠름)</p></li>
<li><p>Ethernet: 노드와 노드사이의 통신을 위한 프로토콜 (보통 RDMA안됨)</p></li>
<li><p>Infiniband: 노드와 노드 사이의 RDMA를 위해 사용되는 네트워크 프로토콜 (이더넷보다 빠름)</p></li>
</ul>
<img width="932" alt="image" src="https://github.com/eagle705/nlp-book/assets/7252598/717f091c-fa52-4771-af5c-e363ca4c84fb">
<p>정리하면 다음과 같고 분산환경학습에서는 NVLink(노드내에서)와 Infiniband(노드간) 두개만 알면 됩니다. 아래 그림은 비교에 대한 도표입니다.</p>
<img width="1026" alt="image" src="https://github.com/eagle705/nlp-book/assets/7252598/fcf25fba-810f-4720-9283-5cf47820407d">
</section>
<section id="mpi-message-passing-interface">
<h2>MPI (Message Passing Interface)<a class="headerlink" href="#mpi-message-passing-interface" title="Permalink to this heading">#</a></h2>
<p>MPI는 HPC에서 분산 시스템 구현을 위해 1990년대 초에 만들어진 표준화된 데이터 통신 라이브러리입니다.
여기서 나오는 메세지 패싱은 두 프로세서가 데이터를 공유하기 위한 방법중 한가지입니다. 데이터를 서로 공유하기 위해 메세지라는 객체를 만들고 객체 안에는 태그라는 데이터가 있어서 식별용도로 사용할 수 있습니다. 딥러닝에서 병렬처리에서는 보통은 메세지패싱 방법을 사용하게 됩니다.</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Message Passing</p></th>
<th class="head"><p>Message Passing vs Shared Memory</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><img width="1007" alt="image" src="https://github.com/eagle705/nlp-book/assets/7252598/6dcb6614-9ed8-4ea0-9f7b-1abe39224234"></p></td>
<td><p><img width="845" alt="image" src="https://github.com/eagle705/nlp-book/assets/7252598/517651e4-6a6b-4b97-89f4-e8dec4ecc1c8"></p></td>
</tr>
</tbody>
</table>
<p>메세지 패싱을 사용할때 주로 쓰는 연산들이 있고 그것에 대한 인터페이스를 정의한 것이 MPI입니다. Open MPI라는 인터페이스를 구현한 오픈소스 라이브러리가 있지만 대부분의 경우 C++로 구현된 NCCL과 GLOO를 사용합니다. 이 라이브러리들은 <a class="reference external" href="https://pytorch.org/docs/stable/distributed.html">torch.distributed</a>에 포함되어 있어서 쉽게 사용할 수 있습니다.</p>
<p><img alt="image" src="https://github.com/eagle705/nlp-book/assets/7252598/9ce68cf4-4058-41e5-a8e8-ce8d6368127b" /></p>
<ul class="simple">
<li><p>NCCL(NVIDIA Collective Communications Library; Nickel로 발음): NVIDIA에서 개발한 collective communication library입니다. GPU 간 통신은 NVLink, PCIe, GPU Direct P2P를 통해 통신하며 노드 간에는 Socket, Infiniband를 통해 통신합니다.</p>
<ul>
<li><p>주로 GPU VRAM간 통신시 사용</p></li>
<li><p>Infiniband with VRAM 지원</p></li>
<li><p>Ethernet with VRAM 지원</p></li>
<li><p>CPU RAM 통신 미지원</p></li>
<li><p>GPU에서 학습할 때 주로 사용함</p></li>
</ul>
</li>
<li><p>GLOO:Facebook(Meta)에서 개발한 collective communication library 대부분의 Linux에서 사용 가능하며 CPU 병렬화시에 권장하고 있습니다.</p>
<ul>
<li><p>Infiniband에서는 IP가 사용가능해야함</p></li>
<li><p>Ethernet with RAM 지원</p></li>
<li><p><a class="reference external" href="https://www.deepspeed.ai/tutorials/zero-offload/">ZeRO처럼 offloading</a>해서 써야할때 유용함</p></li>
</ul>
</li>
</ul>
<section id="point-to-point-p2p">
<h3>Point-to-Point(P2P)<a class="headerlink" href="#point-to-point-p2p" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>하나의 프로세스에서 다른 프로세스로 데이터를 전송하는 것</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">send</span></code> 와 <code class="docutils literal notranslate"><span class="pre">recv</span></code> 함수 또는 즉시 응답하는(immediate counter-parts) <code class="docutils literal notranslate"><span class="pre">isend</span></code> 와 <code class="docutils literal notranslate"><span class="pre">irecv</span></code> 를 사용</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">send/recv</span></code> 는 모두 블로킹 함수</p>
<ul>
<li><p>두 프로세스는 통신이 완료될 때까지 멈춰있음</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">isend/irecv</span></code> 는 논-블로킹 함수</p>
<ul>
<li><p>즉시 응답함. 스크립트는 실행을 계속하고 메소드는 wait() 를 선택할 수 있는 Work 객체를 반환함</p></li>
</ul>
</li>
</ul>
<p><img alt="image" src="https://github.com/eagle705/nlp-book/assets/7252598/640e12dd-0368-46ba-9243-82e5df023849" /></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;블로킹(blocking) 점-대-점 간 통신&quot;&quot;&quot;</span>

<span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">tensor</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="c1"># Send the tensor to process 1</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">send</span><span class="p">(</span><span class="n">tensor</span><span class="o">=</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dst</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Receive tensor from process 0</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">recv</span><span class="p">(</span><span class="n">tensor</span><span class="o">=</span><span class="n">tensor</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Rank &#39;</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="s1">&#39; has data &#39;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;논-블로킹(non-blocking) 점-대-점 간 통신&quot;&quot;&quot;</span>

<span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">req</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">tensor</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="c1"># Send the tensor to process 1</span>
        <span class="n">req</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">isend</span><span class="p">(</span><span class="n">tensor</span><span class="o">=</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dst</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Rank 0 started sending&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Receive tensor from process 0</span>
        <span class="n">req</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">irecv</span><span class="p">(</span><span class="n">tensor</span><span class="o">=</span><span class="n">tensor</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Rank 1 started receiving&#39;</span><span class="p">)</span>
    <span class="n">req</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Rank &#39;</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="s1">&#39; has data &#39;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</section>
<section id="collective-communication">
<h3>Collective Communication 통신(집합통신)<a class="headerlink" href="#collective-communication" title="Permalink to this heading">#</a></h3>
<p>점-대-점 간 통신과 달리 집합 통신은 그룹 의 모든 프로세스에 걸친 통신 패턴을 허용합니다. 그룹은 모든 프로세스의 부분 집합입니다. 그룹을 생성하기 위해서는 dist.new_group(group) 에 순서(rank) 목록을 전달합니다. 기본적으로, 집합 통신은 월드(world) 라고 부르는 전체 프로세스에서 실행됩니다. 예를 들어, 모든 프로세스에 존재하는 모든 Tensor들의 합을 얻기 위해서는 dist.all_reduce(tensor, op, group) 을 사용하면 됩니다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot; All-Reduce 예제 &quot;&quot;&quot;</span>
<span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; 간단한 집합 통신 &quot;&quot;&quot;</span>
    <span class="n">group</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">new_group</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">dist</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Rank &#39;</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="s1">&#39; has data &#39;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
<img width="952" alt="image" src="https://github.com/eagle705/nlp-book/assets/7252598/630e6f86-211b-48ad-b4e6-f02008fe5376">
<p>기본 연산은 다음과 같이 정리할 수 있습니다.</p>
<ul class="simple">
<li><p>broadcast: 복사하기</p>
<ul>
<li><p>특정 rank에 있는 데이터를 다른 통신에 참여하는 모든 rank에 복사</p></li>
</ul>
</li>
<li><p>scatter: 쪼개기</p>
<ul>
<li><p>특정 rank에서 통신에 참여하는 프로세스들에게 데이터를 쪼개서 나눠줌</p></li>
</ul>
</li>
<li><p>gather: 모으기</p>
<ul>
<li><p>여러개의 프로세스에서 갖고 있는 데이터를 모아서 하나의 rank에서 리스트형식으로 조합</p></li>
</ul>
</li>
<li><p>reduction: 연산하기</p>
<ul>
<li><p>똑같이 모으지만 리스트형식으로 쌓는게 아니라 sum, product, max, min등 연산을 통해서 값을 모으는 방법</p></li>
</ul>
</li>
<li><p>All-XXX: 통신 결과를 모든 모든 rank가 갖게 만드는 것</p></li>
<li><p>Reduce-Scatter: 연산한 뒤 결과를 쪼개서 전달</p></li>
</ul>
<img width="1430" alt="image" src="https://github.com/eagle705/nlp-book/assets/7252598/bab97630-00d5-433a-8fc5-0f08bcc87884">
<p>함수형태로 정리하면 다음과 같습니다.</p>
<ul class="simple">
<li><p>dist.broadcast(tensor, src, group): src 의 tensor 를 모든 프로세스의 tensor 에 복사합니다.</p></li>
<li><p>dist.reduce(tensor, dst, op, group): op 를 모든 tensor 에 적용한 뒤 결과를 dst 프로세스의 tensor 에 저장합니다.</p></li>
<li><p>dist.all_reduce(tensor, op, group): 리듀스와 동일하지만, 결과가 모든 프로세스의 tensor 에 저장됩니다.</p></li>
<li><p>dist.scatter(tensor, scatter_list, src, group): i 번째 Tensor scatter_list[i] 를 i 번째 프로세스의 tensor 에 복사합니다.</p></li>
<li><p>dist.gather(tensor, gather_list, dst, group): 모든 프로세스의 tensor 를 dst 프로세스의 gather_list 에 복사합니다.</p></li>
<li><p>dist.all_gather(tensor_list, tensor, group): 모든 프로세스의 tensor 를 모든 프로세스의 tensor_list 에 복사합니다.</p></li>
<li><p>dist.barrier(group): group 내의 모든 프로세스가 이 함수에 진입할 때까지 group 내의 모든 프로세스를 멈춥(block)니다.</p></li>
</ul>
</section>
</section>
<section id="d-parallelism">
<h2>3D Parallelism<a class="headerlink" href="#d-parallelism" title="Permalink to this heading">#</a></h2>
<p>지금부터는 위에서 이야기했던 Collective Communication을 활용한 다양한 병렬화 기법에 대해서 다뤄보도록 하겠습니다. 병렬화 기법은 크게 3가지가 대표적으로 Data Parallelism(DP), Model Parallelism(MP)(혹은 Tensor Parallelism(TP)), Pipeline Parallelism(PP)이 있습니다. 아래 예시는 32장의 GPU로 DP=2, PP=4, TP=4로 분할한 경우입니다.</p>
<img width="944" alt="image" src="https://github.com/eagle705/nlp-book/assets/7252598/bc401b1f-41d9-4cd8-b533-39e5c255ed94">
<section id="data-parallelism-dp">
<h3>Data Parallelism (DP)<a class="headerlink" href="#data-parallelism-dp" title="Permalink to this heading">#</a></h3>
<section id="dp">
<h4>DP 구현<a class="headerlink" href="#dp" title="Permalink to this heading">#</a></h4>
<p>초기 DP의 경우 <code class="docutils literal notranslate"><span class="pre">torch.nn.DataParallel</span></code>에 구현되어 있습니다.</p>
<ul class="simple">
<li><p>싱글 프로세스 + 멀티쓰레드로 구현됨</p></li>
<li><p>싱글 프로세스이므로 멀티노드에서 사용할 수 없고 한대의 컴퓨터에서만 사용할 수 있었음</p></li>
</ul>
<img width="1192" alt="image" src="https://github.com/eagle705/nlp-book/assets/7252598/420b90ff-04f5-4e7c-abc3-3ecac4ddb5a0">
<p>DP에서 Forward pass는 다음과 같이 일어납니다. GPU가 0~3까지 있다고 할때 데이터를 scatter 연산을 통해 분배해줍니다. 그 후 동일한 모델을 GPU-0에 올려놓고 GPU 1,2,3에 broadcast 연산을 통해 동일하게 복사를 해줍니다. 그 후 scatter로 분배된 각각의 데이터에 대해서 Forward를 해주고 logits을 계산하게 됩니다. 각각의 GPU에서 구한 logits을 다시 GPU-0에 gather 연산을 통해 모아주면서 모아진 값을 기반으로 기존에 갖고 있던 label과 비교해서 loss를 구하게 됩니다.
<img width="1603" alt="image" src="https://github.com/eagle705/nlp-book/assets/7252598/c9ec9445-b9bc-420a-ab87-a7925dddd7eb"></p>
<p>Backward pass에서는 전체 데이터에 대한 loss를 먼저 다시 scatter 연산을 통해 각각의 GPU에 분배를 해주고, 분배된 loss를 기반으로 Backward 연산을 통해 Gradient 값들을 계산해줍니다. 각각의 GPU에서 계산된 Gradient를 다시 GPU-0으로 모아서 평균을 계산하고 GPU-0에 있는 모델의 파라미터를 업데이트하게 됩니다. 업데이트 후에 다시 다음 batch에 대해서 Forward를 수행할때 broadcast 연산으로 모델을 각각의 GPU에 복사해주면서 Data Parallelism이 업데이트된 모델에 대해서 수행되게 됩니다.</p>
<img width="1195" alt="image" src="https://github.com/eagle705/nlp-book/assets/7252598/485864b5-e5dd-42d1-b63d-e4975c142d46">
<p>실제로 Forward와 Backward를 비교해보면 Backward의 계산량이 더 큰 편입니다.</p>
<img width="1125" alt="image" src="https://github.com/eagle705/nlp-book/assets/7252598/62f1dde8-8cff-4354-bc21-8c7baba56f90">
<p>이러한 DP의 문제점은 파이썬에서는 GIL(Global Interpreter Lock) 때문에 멀티쓰레드가 효율적이지 않다는 점과 노드간 통신이 불가능하다는 점입니다.  (GIL은 CPU 동작에서 적용되므로 스레드가 I/O 작업을 실행하는 동안에는 다른 스레드가 CPU 동작을 동시에 실행할 수 있기 떄문에 CPU 동작이 비교적 많지 않고 I/O 동작이 더 많은 프로그램에서는 파이썬에서도 멀티스레드가 효과가 있습니다)
<img alt="image" src="https://github.com/eagle705/nlp-book/assets/7252598/c2e0ca56-4b44-451d-906c-5ac214c98313" /></p>
<p>뿐만아니라 한 디바이스(GPU-0)에서만 모델 업데이트가 되는 구조이므로 모델을 매번 복사해줘야되는 추가 비용이 있습니다. 모델이 크면 클 수록 더 큰 통신 비용이 발생하게 되는 구조입니다.</p>
<img width="461" alt="image" src="https://github.com/eagle705/nlp-book/assets/7252598/2cc2ef6d-dc45-4f62-8973-04d8bef55cc3">
<p>이러한 DP 구조를 개선하기 위해서는 각각의 문제를 아래와 같이 개선할 수 있습니다.</p>
<ul class="simple">
<li><p>멀티쓰레드 기반의 구조를 멀티프로세스로 변경해서 개발하는 방법</p></li>
<li><p>한 디바이스의 모델만 업데이트 후 복사하는 방법이 아닌 모든 디바이스에서 계산된 Gradient들의 평균을 계산해서 모든 디바이스의 모델을 업데이트하는 방법 (All-reduce?)</p></li>
</ul>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>멀티프로세스</p></th>
<th class="head"><p>All-Reduce로 Gradient 평균계산</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><img width="1033" alt="image" src="https://github.com/eagle705/nlp-book/assets/7252598/044eb4d3-bffb-44a3-abd8-330a79507948"></p></td>
<td><p><img width="839" alt="image" src="https://github.com/eagle705/nlp-book/assets/7252598/d7da4cfa-540b-4971-adf2-dd156616274a"></p></td>
</tr>
</tbody>
</table>
<p>All-reduce를 통해 업데이트하는 경우 속도가 느린편이기 때문에 2017년 Baidu에서 개발한 Ring All-Reduce를 사용할 수 있습니다. (Uber의 Horovod, NCCL에서 활용되고 TensorFlow, PyTorch(DDP) 등에도 구현)</p>
<p><img alt="ring_allreduce" src="https://github.com/eagle705/nlp-book/assets/7252598/38ad9c47-4afc-45d4-93a3-0e75ee131997" /></p>
<ul class="simple">
<li><p>Ring All-Reduce 추가 설명 (TBD)</p>
<ul>
<li><p>NCCL 실제 구현에서는 Reduce-Scatter + All-Gather와 동일</p></li>
<li><p>All-Reduce를 통해서 보내게되면 각각의 Gradient를 모두 보내서 리스트안에 담아서 더하는 구조기때문에 통신량이 Gradient의 개수가 GPU 개수만큼으로 많지만 Ring All-Reduce에서는 보내기전에 이미 더하고 그 더한값을 보내는 방식이라 보내는 데이터의 개수자체가 좀 더 적은 방식</p></li>
<li><p><a class="reference external" href="https://housekdk.gitbook.io/ml/ml/distributed-training/data-parallelism-overview#ring-all-reduce">참고 예정</a></p></li>
</ul>
</li>
</ul>
<p>이러한 방법들을 채택해서 개발된 것이 DDP (<code class="docutils literal notranslate"><span class="pre">torch.nn.parallel.DistributedDataParallel</span></code>)입니다.
정리한 그림은 아래와 같습니다. 하지만 실제로는 뒤쪽 레이어의 Gradient부터 차례대로 All-Reduce를 진행하며 통신과 Backward를 동시에 수행해서 더 빠르게 계산하는 구조로 진행됩니다.
<img width="1299" alt="image" src="https://github.com/eagle705/nlp-book/assets/7252598/cfc96285-d497-4377-bb45-417776ff5cc1"></p>
<p>속도를 비교하면 다음과 같이 더 빨라지는걸 볼 수 있습니다.
<img alt="image" src="https://github.com/eagle705/nlp-book/assets/7252598/a8db7b97-7ec6-43bc-8488-128989858c0c" /></p>
</section>
</section>
<section id="model-parallelism">
<h3>Model Parallelism<a class="headerlink" href="#model-parallelism" title="Permalink to this heading">#</a></h3>
<p>Model Parallelism은 모델을 여러 GPU로 분할하는 기법을 의미합니다. 하나의 GPU에 올리기에는 큰 모델들에 대해서 MP를 적용하게 됩니다. 이때 학습하는 데이터는 모든 GPU에서 동일합니다.</p>
<img width="1581" alt="image" src="https://github.com/eagle705/nlp-book/assets/7252598/0b4a1257-aa1b-4b9e-ad24-d2e70a53a061">
<section id="mp">
<h4>2가지 MP 기법<a class="headerlink" href="#mp" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>Inter-layer 모델 병렬화</p>
<ul>
<li><p>레이어 단위로 모델을 병렬화 하는 기법</p></li>
</ul>
</li>
<li><p>Intra-layer 모델 병렬화</p>
<ul>
<li><p>행렬 단위로 나눠서 병렬화하는 기법 (ex) 512 dim -&gt; 256, 256 dim)</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1909.08053">NVIDIA의 Megatron-LM 논문</a>에 자세히 기록되어있음</p></li>
</ul>
</li>
</ul>
<img width="1550" alt="image" src="https://github.com/eagle705/nlp-book/assets/7252598/85b0404c-38e1-439e-aebd-29671d07031c">
</section>
<section id="inter-layer">
<h4>Inter-layer 기법의 단점<a class="headerlink" href="#inter-layer" title="Permalink to this heading">#</a></h4>
<img width="999" alt="image" src="https://github.com/eagle705/nlp-book/assets/7252598/7ab47436-b191-4046-899b-e0df9bc870bb">
Inter-Layer 모델 병렬화의 경우에는 배치 입력값이 인공신경망을 레이어별로 통과하는동안 이미 통과한 레이어에서는 더 이상 GPU 연산이 일어나지 않게 되는 idle 상태가 발생한다는 비효율이 존재하는데요. 이러한 비효율을 극복하기 위한 방법으로 Pipeline Parallelism이 등장하게 되었습니다.
</section>
</section>
<section id="pipeline-parallelism-pp">
<h3>Pipeline Parallelism (PP)<a class="headerlink" href="#pipeline-parallelism-pp" title="Permalink to this heading">#</a></h3>
<p>위에서 언급한 단점을 극복하기 위해 GPU가 처리하는 배치(ex. 64 bsz)를 마이크로 배치(ex. 8 bsz)라는 더 작은 단위로 쪼개서 입력으로 넣게됩니다. 마이크로 배치의 크기를 적당한 숫자로 셋팅해주게 되면 더 빠르고 효율적인 GPU 자원활용이 가능합니다.</p>
<img width="1265" alt="image" src="https://github.com/eagle705/nlp-book/assets/7252598/07a4a4cc-d1ba-4d60-92ef-4ef59c0ebae8">
<img width="1248" alt="image" src="https://github.com/eagle705/nlp-book/assets/7252598/56de34ff-5434-486c-9a93-4d7fe826e2bc">
<p>하지만 위와 같은 형태의 PP 또한 단점이 있는데요. 마이크로배치로 쪼개서 효율을 추구했지만, 그 다음 배치가 입력으로 들어올때 GPU가 쉬는 bubble이 존재한다는 것 그리고 모든 마이크로배치에 대해 forward 후 backward 할때까지 Activation memory를 유지해야한다는 단점이 있습니다.</p>
<p>여기서 잠깐! Activation Memory가 뭘까요?
실제 backward를 하기위해서는 forward 하는 시점에 값들을 저장해놔야합니다. 실제 파이토치의 autograd.Function의 구현을 보면 아래와 같이 값을 저장하고 있습니다.
<img width="1417" alt="image" src="https://github.com/eagle705/nlp-book/assets/7252598/08c35441-a6a6-41b0-821e-44a0a5dff920"></p>
<p>Activation memory를 줄이는 방법은 1F1B(1 Forward, 1 Backward)라는 방식을 사용하는 것입니다. 모든 마이크로배치가 끝난 후 Backward를 하는게 아니라 마이크로배치단위로 backward를 바로 실행해주면 Activation memory를 줄일 수 있습니다. 이런 방법을 사용하면 대략 2배정도까지 메모리를 아낄 수 있습니다. PipeDream-Flush가 대표적이며 DeepSpeed, Megatron-LM등 유명한 프레임워크에서도 사용하고 있습니다.</p>
<img width="638" alt="image" src="https://github.com/eagle705/nlp-book/assets/7252598/e264fdb9-59cb-433a-a8e2-1cdb5fbe58c3">
<section id="intra-layer">
<h4>Intra-layer 기법 알아보기<a class="headerlink" href="#intra-layer" title="Permalink to this heading">#</a></h4>
<p>Intra layer 병렬화는 Tensor Model Parallelism (TP)이라고도 부릅니다. 행렬곱 연산의 성질을 이용하면 병렬화 전후의 연산결과를 동일하게 만들 수 있습니다.</p>
</section>
<section id="column-parallelism">
<h4>Column Parallelism<a class="headerlink" href="#column-parallelism" title="Permalink to this heading">#</a></h4>
<p>입력 데이터 X는 GPU마다 복사해주고, 입력에 곱해지는 모델 파라미터 행렬 W를 수직(column)으로 쪼갠 뒤 곱해준후 다시 concat해줌으로써 쪼개기 전과 동일한 결과 값을 얻을 수 있습니다.</p>
<img width="1305" alt="image" src="https://github.com/eagle705/nlp-book/assets/7252598/49e9086b-cb2f-4904-9ecb-6765c0bbc3b4">
</section>
<section id="row-parallelism">
<h4>Row Parallelism<a class="headerlink" href="#row-parallelism" title="Permalink to this heading">#</a></h4>
<p>입력 데이터는 수직(column)으로 쪼개고 모델 파라미터 행렬 W는 수평(row)로 쪼갠 뒤 더해주는 방식으로 진행하면 쪼개기 전과 동일한 결과 값을 얻을 수 있습니다.
<img width="1315" alt="image" src="https://github.com/eagle705/nlp-book/assets/7252598/a6c7f75f-339d-4d54-87b2-99fa00d92ad0"></p>
</section>
<section id="mlp">
<h4>MLP 연산<a class="headerlink" href="#mlp" title="Permalink to this heading">#</a></h4>
<p>MLP의 경우엔 중간에 Activation이 있기 때문에 Addition이 들어가지 않는 Column Parallelism을 먼저 사용하게 되고 그 결과를 바로 활용할 수 있는 Row Parallelism을 이어 붙임으로써 빠르게 구현할 수 있습니다
Column -&gt; Row 순으로 연결하면 빠르게 구현이 가능함 (All-gather와 Scatter 연산 생략 가능)</p>
<img width="1555" alt="image" src="https://github.com/eagle705/nlp-book/assets/7252598/a2cfbff9-d8fb-439f-a882-553171d00fe8">
<p><img alt="image" src="https://github.com/eagle705/nlp-book/assets/7252598/919a011f-d320-4818-bcf8-9bb1a1d168ec" /></p>
</section>
<section id="attention-layer">
<h4>Attention Layer 연산<a class="headerlink" href="#attention-layer" title="Permalink to this heading">#</a></h4>
<p>Attention의 경우에도 QKV는 Column으로 쪼개고 dropout쪽은 Row로 쪼개서 진행할 수 있습니다.</p>
</section>
<section id="embedding-layer">
<h4>Embedding Layer 연산<a class="headerlink" href="#embedding-layer" title="Permalink to this heading">#</a></h4>
<p>임베딩의 경우 vocab size를 기준으로 병렬화를 진행합니다. 입력데이터가 들어왔을때 각 GPU에서 처리할 수 있는 vocab에 대해서만 처리하고 나머지는 MASK 처리를 하게 됩니다. 병렬처리 되어 있는 Embedding Layer에서 각각의 vocab index에 대한 Embedding을 꺼내온 후 All-reduce (sum)을 하게 되면 원래 의도했던 vocab index와 대응되는 전체 Embedding matrix를 얻을 수 있게 됩니다.
<img width="1364" alt="image" src="https://github.com/eagle705/nlp-book/assets/7252598/392fce65-f539-4e25-be10-3b373182117f"></p>
</section>
</section>
</section>
<section id="gradient-accumulation">
<h2>Gradient Accumulation<a class="headerlink" href="#gradient-accumulation" title="Permalink to this heading">#</a></h2>
<p>위와 같은 다양한 Parallism 기법을 적용해도 GPU 메모리가 부족해서 batch size를 키우기 어려운 경우가 생깁니다. 즉 우리가 원하는 만큼의 데이터를 한번에 학습하기 어려운 경우가 생깁니다. 이런 경우에는 매 스텝마다 모델의 파라미터를 업데이트하지않고 parameter.grad 텐서에서 여러 backward 연산의 gradient들을 모았다가 일정 스텝이 지나면 파라미터를 업데이트하는 방법이 있습니다. 이를 통해 큰 batch size로 모델의 파라미터를 업데이트하는 효과를 낼 수 있으며 이러한 기법을 Gradient Accumulation이라 부릅니다.</p>
<p><img alt="image" src="https://github.com/eagle705/nlp-book/assets/7252598/da4ac170-c3f3-49e5-8905-9b06bf8409db" /></p>
<p>이때 loss도 합산되므로 <code class="docutils literal notranslate"><span class="pre">accumulation_steps</span></code>으로 나누어 주어야합니다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>                                   <span class="c1"># Reset gradients tensors</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">training_set</span><span class="p">):</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>                     <span class="c1"># Forward pass</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_function</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>       <span class="c1"># Compute loss function</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">/</span> <span class="n">accumulation_steps</span>                <span class="c1"># Normalize our loss (if averaged)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>                                 <span class="c1"># Backward pass</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">accumulation_steps</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>             <span class="c1"># Wait for several backward steps</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>                            <span class="c1"># Now we can do an optimizer step</span>
        <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>                           <span class="c1"># Reset gradients tensors</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">evaluation_steps</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>           <span class="c1"># Evaluate the model when we...</span>
            <span class="n">evaluate_model</span><span class="p">()</span>                        <span class="c1"># ...have no gradients accumulated</span>
</pre></div>
</div>
</section>
<section id="gradient-checkpointing">
<h2>Gradient Checkpointing<a class="headerlink" href="#gradient-checkpointing" title="Permalink to this heading">#</a></h2>
<p>메모리가 부족한 경우, 속도는 조금 느려지더라도 메모리 사용량을 더 확보하기 위해 적용하는 기법중 하나이며 모든 노드에 gradient를 저장하지 않고 일부 노드에만 gardient를 저장함하는 기법이다.</p>
<ul class="simple">
<li><p>Ref: <a class="reference external" href="https://only-wanna.tistory.com/entry/Gradient-checkpointing%EC%9D%B4%EB%9E%80">https://only-wanna.tistory.com/entry/Gradient-checkpointing이란</a></p></li>
</ul>
</section>
<section id="nvidia-apex">
<h2>NVIDIA Apex<a class="headerlink" href="#nvidia-apex" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>TBD</p></li>
</ul>
</section>
<section id="reference">
<h2>Reference<a class="headerlink" href="#reference" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>PyTorch 공식 튜토리얼</p></li>
<li><p><a class="reference external" href="https://nbviewer.org/github/tunib-ai/large-scale-lm-tutorials/blob/main/notebooks/07_tensor_parallelism.ipynb">https://nbviewer.org/github/tunib-ai/large-scale-lm-tutorials/blob/main/notebooks/07_tensor_parallelism.ipynb</a></p></li>
<li><p><a class="reference external" href="https://velog.io/&#64;miracle-21/PYTHONGIL-Multi-Thread">https://velog.io/&#64;miracle-21/PYTHONGIL-Multi-Thread</a></p></li>
<li><p><a class="reference external" href="https://housekdk.gitbook.io/ml/ml/distributed-training/data-parallelism-overview">https://housekdk.gitbook.io/ml/ml/distributed-training/data-parallelism-overview</a></p></li>
<li><p><a class="reference external" href="https://algopoolja.tistory.com/95">https://algopoolja.tistory.com/95</a></p></li>
<li><p><a class="reference external" href="https://only-wanna.tistory.com/entry/Gradient-checkpointing%EC%9D%B4%EB%9E%80">https://only-wanna.tistory.com/entry/Gradient-checkpointing이란</a></p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./LLM"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

            </article>
            

            
            
            <footer class="bd-footer-article">
                <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            </footer>
            
          </div>
          
          
          
            <div class="bd-sidebar-secondary bd-toc">
              
<div class="toc-item">
  
<div class="tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
</div>
<nav id="bd-toc-nav" class="page-toc">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   용어
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#node-gpu">
     Node &amp; GPU
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hw">
     HW
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   분산학습에 쓰이는 통신기술
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#mpi-message-passing-interface">
   MPI (Message Passing Interface)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#point-to-point-p2p">
     Point-to-Point(P2P)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#collective-communication">
     Collective Communication 통신(집합통신)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#d-parallelism">
   3D Parallelism
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#data-parallelism-dp">
     Data Parallelism (DP)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#dp">
       DP 구현
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-parallelism">
     Model Parallelism
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#mp">
       2가지 MP 기법
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#inter-layer">
       Inter-layer 기법의 단점
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pipeline-parallelism-pp">
     Pipeline Parallelism (PP)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#intra-layer">
       Intra-layer 기법 알아보기
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#column-parallelism">
       Column Parallelism
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#row-parallelism">
       Row Parallelism
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#mlp">
       MLP 연산
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#attention-layer">
       Attention Layer 연산
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#embedding-layer">
       Embedding Layer 연산
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gradient-accumulation">
   Gradient Accumulation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gradient-checkpointing">
   Gradient Checkpointing
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#nvidia-apex">
   NVIDIA Apex
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#reference">
   Reference
  </a>
 </li>
</ul>

</nav>
</div>

            </div>
          
          
        </div>
        <footer class="bd-footer-content">
          <div class="bd-footer-content__inner">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By JSYoon
</p>

  </div>
  
  <div class="footer-item">
    
<p class="copyright">

    &copy; Copyright 2022.<br>

</p>

  </div>
  
  <div class="footer-item">
    <p class="last-updated">
Last updated on None.<br>
</p>
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </div>
        </footer>
        

      </main>
    </div>
  </div>

  
    
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=796348d33e8b1d947c94"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=796348d33e8b1d947c94"></script>

  </body>
</html>